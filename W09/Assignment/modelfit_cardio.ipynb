{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxGJbLsUhuc8"
   },
   "source": [
    "## Subash Chandra Biswal (U77884251) ##\n",
    "# WE08 - Cardiotocography\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tuXRZKEYrDa"
   },
   "source": [
    "## Introduction and Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q08EVUytY3eh"
   },
   "source": [
    "Author: J. P. Marques de SÃ¡, J. Bernardes, D. Ayers de Campos.  \n",
    "Source: UCI  \n",
    "Please cite: Ayres de Campos et al. (2000) SisPorto 2.0 A Program for Automated Analysis of Cardiotocograms. J Matern Fetal Med 5:311-318, UCI    \n",
    "\n",
    "2126 fetal cardiotocograms (CTGs) were automatically processed and the respective diagnostic features measured. The CTGs were also classified by three expert obstetricians and a consensus classification label assigned to each of them. Classification was both with respect to a morphologic pattern (A, B, C. ...) and to a fetal state (N, S, P). Therefore the dataset can be used either for 10-class or 3-class experiments.  \n",
    "\n",
    "Attribute Information:  \n",
    "LB - FHR baseline (beats per minute)  \n",
    "AC - # of accelerations per second  \n",
    "FM - # of fetal movements per second  \n",
    "UC - # of uterine contractions per second  \n",
    "DL - # of light decelerations per second  \n",
    "DS - # of severe decelerations per second  \n",
    "DP - # of prolongued decelerations per second  \n",
    "ASTV - percentage of time with abnormal short term variability  \n",
    "MSTV - mean value of short term variability  \n",
    "ALTV - percentage of time with abnormal long term variability  \n",
    "MLTV - mean value of long term variability  \n",
    "Width - width of FHR histogram  \n",
    "Min - minimum of FHR histogram  \n",
    "Max - Maximum of FHR histogram  \n",
    "Nmax - # of histogram peaks  \n",
    "Nzeros - # of histogram zeros  \n",
    "Mode - histogram mode  \n",
    "Mean - histogram mean  \n",
    "Median - histogram median  \n",
    "Variance - histogram variance  \n",
    "Tendency - histogram tendency  \n",
    "CLASS - FHR pattern class code (1 to 10)  \n",
    "NSP - fetal state class code (N=normal(1); S=suspect(2); P=pathologic(3))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmYLcm3aY8X5"
   },
   "source": [
    "## Install and import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8zNdljvIhuc8"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,classification_report\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.initializers import GlorotNormal\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "# set random seed to ensure that results are repeatable\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGgrXNQPZT3J"
   },
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "q3u5LsGyhudA"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"./X_train.csv\")\n",
    "y_train = pd.read_csv(\"./y_train.csv\")\n",
    "X_test = pd.read_csv(\"./X_test.csv\")\n",
    "y_test = pd.read_csv(\"./y_test.csv\")\n",
    "X = pd.read_csv(\"./X.csv\")\n",
    "y = pd.read_csv(\"./y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics ##\n",
    "Since this is pharmacutical data and we are targeting suspects from various medical test data, we need to minimize the false negatives as this will cost somebody's life. This cost is significantly high as compared to false positive and in case of false positive the patient/insurance company needs to bear only the further investigation costs. \n",
    "\n",
    "Since this is a classification problem our score metrics is confusion matix and our measure of score is recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "svmm = SVC()\n",
    "logreg = LogisticRegression()\n",
    "adatree = AdaBoostClassifier()\n",
    "rforest = RandomForestClassifier()\n",
    "xgboost = XGBClassifier()\n",
    "gboost = GradientBoostingClassifier()\n",
    "ann = MLPClassifier()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Random search of parameter grids of all models ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "The best recall score is 0.9539958592132505\n",
      "... with parameters: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 306}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "70 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.95399586 0.61486542 0.95395445 0.54302277        nan 0.62062112\n",
      "        nan 0.7757764  0.90786749 0.64078675 0.40807453 0.87349896\n",
      " 0.37921325        nan 0.89639752 0.40517598 0.95395445 0.77271222\n",
      " 0.95395445 0.95399586 0.8389648  0.95399586 0.83304348 0.95399586\n",
      " 0.61486542 0.61486542 0.70115942 0.76993789 0.7126294  0.95399586\n",
      " 0.71254658 0.63213251 0.90496894 0.95399586 0.95395445 0.95395445\n",
      " 0.61776398 0.89929607        nan 0.62637681 0.63213251 0.95395445\n",
      " 0.75296066 0.46277433 0.95399586 0.63213251        nan 0.63213251\n",
      "        nan 0.95399586 0.5257971  0.95395445 0.59763975 0.95395445\n",
      " 0.95395445 0.63213251        nan        nan 0.50567288 0.95399586\n",
      " 0.95395445 0.51142857 0.63792961 0.95395445 0.62637681 0.83296066\n",
      " 0.62923395 0.41966874 0.57461698        nan 0.62637681 0.84467909\n",
      " 0.7384265  0.72720497 0.52869565 0.64078675 0.63503106 0.39937888\n",
      " 0.83590062        nan 0.77859213 0.95399586        nan 0.63213251\n",
      "        nan 0.95399586 0.49995859 0.95399586 0.62923395 0.43122153\n",
      " 0.67246377 0.60915114 0.57461698 0.95399586 0.62347826        nan\n",
      "        nan 0.79855072 0.95395445 0.6378882 ]\n",
      "  warnings.warn(\n",
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [0.97126686 0.61494546 0.96623604 0.54166989        nan 0.61925685\n",
      "        nan 0.78736495 0.90373379 0.64367861 0.42169619 0.88721281\n",
      " 0.38793739        nan 0.91020345 0.41380315 0.96623604 0.79742657\n",
      " 0.96623604 0.97055001 0.85631108 0.97055001 0.8527449  0.97126686\n",
      " 0.61566231 0.61279235 0.70976767 0.77871638 0.7076094  0.97126686\n",
      " 0.72701839 0.63649983 0.9023001  0.97126686 0.96623604 0.96623604\n",
      " 0.61925685 0.8972796         nan 0.62931848 0.63506356 0.96623604\n",
      " 0.75500632 0.46910858 0.97055001 0.63578299        nan 0.63793868\n",
      "        nan 0.97126686 0.52658776 0.96623604 0.59052113 0.96623604\n",
      " 0.96623604 0.63362471        nan        nan 0.51006679 0.97055001\n",
      " 0.96623604 0.52012583 0.64224234 0.96623604 0.63003791 0.85418375\n",
      " 0.6314716  0.43606147 0.58118666        nan 0.62931848 0.86137284\n",
      " 0.73276862 0.74638354 0.52874088 0.64367603 0.6400918  0.40374926\n",
      " 0.84843093        nan 0.78231866 0.97126686        nan 0.63506356\n",
      "        nan 0.97055001 0.49498208 0.97126686 0.63290529 0.44755421\n",
      " 0.6989892  0.605611   0.58190351 0.97126686 0.62356824        nan\n",
      "        nan 0.81899384 0.96623604 0.64511488]\n",
      "  warnings.warn(\n",
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Grid for Logistic Regression\n",
    "\n",
    "param_grid_logr = [{\n",
    "     'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "     'solver': ['saga'],\n",
    "     'max_iter': np.arange(100,900),},\n",
    "      {\n",
    "     'penalty': ['l1', 'l2'],\n",
    "     'solver': ['liblinear'],\n",
    "     'max_iter': np.arange(100,900),},\n",
    "    {\n",
    "     'penalty': ['l2', 'none'],\n",
    "     'solver': ['lbfgs'],\n",
    "     'max_iter': np.arange(100,900),}\n",
    "]    \n",
    "\n",
    "rand_search_logr = RandomizedSearchCV(estimator = logreg, param_distributions=param_grid_logr, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Logistic Regression model fit for grid search\n",
    "_ = rand_search_logr.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_logr.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_logr.best_params_}\")\n",
    "\n",
    "bestRecallLogr = rand_search_logr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "The best recall score is 0.9453416149068324\n",
      "... with parameters: {'min_samples_split': 31, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.0021, 'max_leaf_nodes': 43, 'max_depth': 15, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "5 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 889, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 177, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.80198758 0.62335404 0.9252588  0.8852588  0.71536232 0.61192547\n",
      " 0.62335404 0.90815735 0.72401656 0.84186335 0.80488613 0.79039337\n",
      " 0.88530021 0.92815735 0.92530021 0.92815735 0.92244306 0.86484472\n",
      " 0.85341615 0.91101449 0.84766046 0.7152381  0.74443064 0.6578882\n",
      " 0.71536232 0.90530021 0.85341615 0.73809524 0.6405383  0.86815735\n",
      " 0.80488613 0.80488613 0.72401656 0.91101449 0.92815735 0.79888199\n",
      " 0.79039337 0.60592133 0.8389234  0.60592133 0.85668737 0.67755694\n",
      " 0.90815735 0.8447205         nan 0.79018634 0.79039337 0.92530021\n",
      " 0.94534161 0.69258799 0.65503106 0.90815735 0.71536232 0.66091097\n",
      " 0.77875776 0.73809524 0.67755694 0.77875776 0.71250518 0.88530021\n",
      " 0.87929607 0.73809524 0.79304348 0.65763975 0.73238095 0.81031056\n",
      " 0.91101449 0.90811594 0.61478261 0.91101449 0.92244306 0.73238095\n",
      " 0.84186335 0.80488613 0.91101449 0.71250518 0.85378882 0.78186335\n",
      " 0.87929607 0.81888199 0.80488613 0.6405383  0.6405383  0.8131677\n",
      " 0.91097308 0.80198758 0.60592133 0.81031056 0.71536232 0.62612836\n",
      " 0.90530021 0.85341615 0.7184265  0.72401656 0.82178054 0.71817805\n",
      " 0.89942029 0.79039337 0.61478261 0.79039337]\n",
      "  warnings.warn(\n",
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [0.81172739 0.64867074 0.94612052 0.88717155 0.71410742 0.62131714\n",
      " 0.64867074 0.92383899 0.72911219 0.85631882 0.81316108 0.78950517\n",
      " 0.88786777 0.93391094 0.94038576 0.93391094 0.94395194 0.87643434\n",
      " 0.8599211  0.92455842 0.88071736 0.76287099 0.76649906 0.66886878\n",
      " 0.71554369 0.91160878 0.86279879 0.75652252 0.65952399 0.88499007\n",
      " 0.81316108 0.81316108 0.72911219 0.92455842 0.93391094 0.81243392\n",
      " 0.78590805 0.6271731  0.85419922 0.62645368 0.86127227 0.71054124\n",
      " 0.92383899 0.86637787        nan 0.81098734 0.78950517 0.94038576\n",
      " 0.95833011 0.6938604  0.66167453 0.92383899 0.71410742 0.69028132\n",
      " 0.78447693 0.75652252 0.71054124 0.78447693 0.70547175 0.88786777\n",
      " 0.88505969 0.75652252 0.8117145  0.6709729  0.76083907 0.81680462\n",
      " 0.92455842 0.91953018 0.61843944 0.92455842 0.93463036 0.76083907\n",
      " 0.85631882 0.81316108 0.92455842 0.7097883  0.86198912 0.80232072\n",
      " 0.88505969 0.82471829 0.81316108 0.65952399 0.65952399 0.81895516\n",
      " 0.9288827  0.81172739 0.62645368 0.81680462 0.71410742 0.64579304\n",
      " 0.91160878 0.8599211  0.72839277 0.72911219 0.84196127 0.75713365\n",
      " 0.91164488 0.78950517 0.61843944 0.78590805]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Grid for decision tree\n",
    "param_grid_tree = {\n",
    "    'min_samples_split': np.arange(1,100),  \n",
    "    'min_samples_leaf': np.arange(1,100),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 50), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "rand_search_tree = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid_tree, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Decision tree model fit for grid search\n",
    "_ = rand_search_tree.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_tree.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_tree.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 15 is smaller than n_iter=50. Running 15 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.9712629399585921\n",
      "... with parameters: {'kernel': 'linear', 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "# Grid for SVM\n",
    "param_grid_svm = [{\n",
    "    'degree': [2,3],\n",
    "    'C': [1,5,10],\n",
    "    'kernel': ['poly'],   \n",
    "},\n",
    "{\n",
    "    'C': [1,5,10],\n",
    "    'gamma': [1, 0.1],\n",
    "    'kernel': ['rbf'],   \n",
    "},\n",
    "{\n",
    "    'C': [1,5,10],\n",
    "    'kernel': ['linear'],  \n",
    "}]\n",
    "\n",
    "rand_search_svm = RandomizedSearchCV(estimator = svmm, param_distributions=param_grid_svm, cv=kfolds, n_iter=50,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# SVM model fit for grid search\n",
    "_ = rand_search_svm.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_svm.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_svm.best_params_}\")\n",
    "\n",
    "bestRecallSvm = rand_search_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 20 is smaller than n_iter=500. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.959792960662526\n",
      "... with parameters: {'n_estimators': 1000, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#Grid for ADABoost Classifier\n",
    "\n",
    "param_grid_ada = {  \n",
    "     'n_estimators': [10,50,250,1000,2000],\n",
    "     'learning_rate': [0.01,0.1,0.2,1.0],}   \n",
    "\n",
    "rand_search_ada = RandomizedSearchCV(estimator = adatree, param_distributions=param_grid_ada, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# ADABoost Classifier model fit for grid search\n",
    "_ = rand_search_ada.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_ada.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_ada.best_params_}\")\n",
    "\n",
    "bestRecallAda = rand_search_ada.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 180 is smaller than n_iter=500. Running 180 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:909: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n",
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.959792960662526\n",
      "... with parameters: {'n_estimators': 250, 'max_features': 'auto', 'max_depth': 10, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "#Grid for Randomforest Classifier\n",
    "\n",
    "param_grid_rf = {  \n",
    "     'n_estimators': [10,50,250,1000,2000],\n",
    "     'max_features': ['auto', 'sqrt', 'log2'],\n",
    "     'max_depth' : [4,6,8,10],\n",
    "     'criterion' :['gini', 'entropy', 'log_loss'],}   \n",
    "\n",
    "rand_search_rf = RandomizedSearchCV(estimator = rforest, param_distributions=param_grid_rf, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# ADABoost Classifier model fit for grid search\n",
    "_ = rand_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_rf.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_rf.best_params_}\")\n",
    "\n",
    "bestRecallRf = rand_search_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:305: UserWarning: The total space of parameters 200 is smaller than n_iter=500. Running 200 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "The best recall score is 0.9655072463768116\n",
      "... with parameters: {'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.2}\n"
     ]
    }
   ],
   "source": [
    "#Grid for XGBoost Classifier\n",
    "\n",
    "param_grid_xg = {  \n",
    "    'max_depth': range (2, 10, 1),\n",
    "    'n_estimators': [10,50,250,1000,2000],\n",
    "    'learning_rate': [1.0,0.2,0.1, 0.01, 0.05],}   \n",
    "\n",
    "rand_search_xg = RandomizedSearchCV(estimator = xgboost, param_distributions=param_grid_xg, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# XGBoost Classifier model fit for grid search\n",
    "_ = rand_search_xg.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_xg.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_xg.best_params_}\")\n",
    "\n",
    "bestRecallXg = rand_search_xg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "160 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "160 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 420, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of GradientBoostingClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.84480331 0.93681159 0.95403727 0.85304348 0.90811594 0.9626087\n",
      " 0.9252588  0.95975155 0.95693582 0.95979296 0.95689441 0.79581781\n",
      " 0.9252588  0.95689441 0.95407867        nan 0.9626501         nan\n",
      " 0.93109731 0.84782609 0.95407867 0.         0.88223602 0.95979296\n",
      " 0.88782609 0.90811594 0.92815735 0.9252588  0.95403727 0.95979296\n",
      " 0.94248447 0.9626501  0.95122153 0.92240166 0.9626087  0.90811594\n",
      "        nan 0.94828157 0.95975155 0.         0.95689441 0.95979296\n",
      " 0.94828157 0.84753623 0.9252588  0.95689441 0.95979296        nan\n",
      " 0.95399586 0.78691511 0.         0.91101449 0.91101449 0.95689441\n",
      " 0.93971014 0.92815735 0.9310559  0.93966874 0.93971014 0.91378882\n",
      " 0.95399586 0.91101449        nan 0.95975155 0.95689441 0.956853\n",
      " 0.90811594 0.87933747 0.95697723 0.92815735        nan 0.80447205\n",
      " 0.91101449 0.95979296 0.93681159 0.95689441 0.92815735 0.9626501\n",
      " 0.92815735 0.93101449 0.9626501  0.9052588  0.79884058 0.93677019\n",
      " 0.95979296 0.         0.95979296 0.95689441 0.92815735 0.91101449\n",
      " 0.9626501  0.94550725 0.95399586 0.90803313 0.93101449 0.92236025\n",
      " 0.95979296 0.90795031 0.95979296        nan 0.95118012 0.9252588\n",
      " 0.95689441 0.90811594 0.95975155        nan 0.95403727 0.92530021\n",
      " 0.84753623 0.95689441 0.90811594 0.93109731 0.92815735 0.\n",
      " 0.95979296 0.92815735 0.93391304 0.95403727 0.93391304 0.32741201\n",
      " 0.95118012 0.95403727 0.94542443 0.91950311 0.88782609 0.94538302\n",
      "        nan        nan 0.         0.80447205 0.90811594        nan\n",
      " 0.9310559  0.         0.84182195 0.         0.91950311 0.93387164\n",
      " 0.95689441 0.95693582 0.95689441 0.89942029 0.9626501  0.48571429\n",
      " 0.95979296 0.95693582 0.95979296 0.91097308 0.92530021 0.95118012\n",
      " 0.90811594 0.9310559  0.92819876 0.90517598 0.94538302 0.91101449\n",
      " 0.84753623 0.96269151 0.95693582 0.92807453 0.93101449 0.95979296\n",
      " 0.87933747 0.90811594 0.9252588  0.9626501  0.93387164 0.95979296\n",
      " 0.95979296        nan 0.         0.95403727 0.9626501  0.93109731\n",
      " 0.9626501  0.95693582 0.93677019 0.92815735 0.92240166 0.92240166\n",
      " 0.9626501  0.95979296 0.95979296 0.91668737 0.95403727 0.92815735\n",
      " 0.9626501  0.93962733 0.9626501  0.95693582 0.93677019 0.94828157\n",
      "        nan 0.8821118  0.95399586 0.92236025 0.9626501  0.93101449\n",
      " 0.91954451 0.95971014 0.95403727 0.93097308 0.92240166 0.95689441\n",
      " 0.96554865 0.         0.91084886 0.95979296 0.9626501  0.95693582\n",
      " 0.91101449 0.93395445 0.95975155 0.8821118  0.9252588  0.91954451\n",
      " 0.90811594 0.93395445 0.94828157 0.95693582 0.91101449 0.95693582\n",
      " 0.93391304        nan 0.95979296 0.90811594 0.91101449 0.936853\n",
      " 0.92815735        nan 0.         0.91950311 0.95975155 0.92244306\n",
      " 0.89937888 0.83602484 0.78438923 0.95975155 0.91101449 0.70703934\n",
      "        nan 0.94546584 0.95403727 0.93966874 0.84753623 0.95975155\n",
      " 0.90811594 0.91954451 0.95403727 0.94252588        nan 0.95403727\n",
      " 0.95399586 0.         0.91374741 0.90811594 0.76426501 0.89656315\n",
      " 0.91954451 0.91101449 0.31312629 0.91101449 0.87942029        nan\n",
      " 0.92252588 0.92240166 0.52033126 0.89080745 0.93966874 0.9310559\n",
      " 0.94256729 0.         0.95689441 0.92530021 0.         0.95399586\n",
      " 0.95693582 0.96550725 0.84484472 0.                nan 0.94828157\n",
      " 0.92244306        nan 0.93975155 0.9252588  0.95979296 0.95975155\n",
      " 0.91097308 0.         0.95113872 0.95979296 0.9626501  0.90231884\n",
      " 0.84467909 0.91101449 0.95118012 0.9626501  0.94828157 0.9252588\n",
      " 0.9626501  0.95113872 0.95689441 0.90811594 0.84198758 0.95403727\n",
      " 0.95113872 0.91101449 0.95122153 0.95979296 0.95975155 0.93378882\n",
      " 0.95689441 0.95399586 0.8994617  0.9252588  0.95693582 0.95979296\n",
      " 0.94542443 0.93677019 0.92815735        nan 0.95979296 0.96550725\n",
      " 0.91101449 0.88496894 0.9252588  0.94542443 0.93399586 0.9252588\n",
      "        nan        nan 0.9252588  0.93966874 0.88795031 0.85908903\n",
      " 0.95113872 0.9252588  0.91101449 0.956853   0.95118012 0.95979296\n",
      "        nan 0.95689441        nan 0.89656315 0.91101449 0.92815735\n",
      " 0.95689441 0.93391304 0.91950311 0.91089027 0.93966874 0.95693582\n",
      " 0.94824017 0.95681159 0.93677019 0.90811594 0.91101449 0.95118012\n",
      " 0.91954451 0.91101449 0.95975155 0.45995859 0.91950311 0.90811594\n",
      " 0.956853   0.90811594 0.95979296 0.49987578 0.9626501  0.94542443\n",
      " 0.91954451 0.75863354 0.93391304 0.94824017 0.95689441 0.95399586\n",
      " 0.95407867 0.94828157 0.95979296 0.95403727 0.91101449 0.95975155\n",
      " 0.96269151 0.95399586 0.93101449 0.93958592 0.91383023 0.95975155\n",
      " 0.95693582 0.95979296 0.95689441 0.87362319 0.53175983 0.95399586\n",
      " 0.92815735 0.95407867 0.84753623 0.9252588  0.95975155 0.80699793\n",
      " 0.90807453 0.91954451 0.90811594 0.91101449 0.91954451 0.95979296\n",
      " 0.94252588 0.90811594 0.87068323 0.9252588  0.95979296 0.95689441\n",
      " 0.9252588  0.         0.95979296 0.93101449 0.9252588  0.95689441\n",
      " 0.9252588  0.9252588  0.95693582 0.81594203        nan 0.9252588\n",
      " 0.9626087  0.88513458 0.90811594 0.95403727 0.91101449        nan\n",
      " 0.94824017        nan 0.95689441 0.91101449 0.92244306 0.79296066\n",
      " 0.9252588  0.85908903 0.93101449 0.91101449 0.         0.48571429\n",
      " 0.90198758 0.95403727 0.93101449 0.95979296 0.87639752 0.93681159\n",
      " 0.93387164 0.91677019 0.9252588  0.92240166 0.92236025        nan\n",
      " 0.91660455 0.92240166 0.87648033 0.95975155 0.95689441 0.95975155\n",
      " 0.93954451 0.92815735 0.9194617  0.         0.93966874 0.94252588\n",
      " 0.90517598 0.956853   0.         0.95693582 0.89652174        nan\n",
      " 0.91954451 0.94538302 0.95407867        nan 0.93097308 0.95693582\n",
      " 0.91954451 0.8821118  0.9510559  0.94828157        nan 0.91093168\n",
      " 0.9252588  0.9626501  0.95979296 0.94542443 0.94538302 0.95979296\n",
      " 0.92815735 0.80111801 0.9252588  0.95407867 0.93681159 0.94538302\n",
      " 0.90811594 0.89656315]\n",
      "  warnings.warn(\n",
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [0.86137799 0.95474072 0.99712746 0.87365205 0.92096903 0.99641061\n",
      " 0.92959697 0.99784431 0.99784431 0.99784431 0.99353807 0.81250097\n",
      " 0.92959697 0.99712746 1.                nan 0.99928315        nan\n",
      " 0.95903664 0.87208685 0.97629509 0.         0.89726155 0.99712746\n",
      " 0.91021119 0.92384157 0.93391094 0.92816328 0.99712746 0.99712746\n",
      " 0.95761326 0.99209922 0.98347387 0.92959697 0.99712746 0.92240788\n",
      "        nan 0.96622831 0.99712746 0.         0.99712746 0.99784431\n",
      " 0.99856373 0.87287331 0.92959697 0.99712746 0.99784431        nan\n",
      " 0.97485882 0.8032387  0.         0.92455842 0.92455842 0.99712746\n",
      " 0.95760553 0.93606921 0.94396741 0.95761069 0.95975864 0.92456615\n",
      " 0.98922153 0.92455842        nan 0.99712746 0.99712746 0.99928058\n",
      " 0.92096903 0.90159872 1.         0.93750032        nan 0.83552513\n",
      " 0.92455842 0.99712746 0.94826332 0.99712746 0.93318893 0.99856373\n",
      " 0.93031381 0.93893917 0.99425234 0.91377994 0.80388592 0.94972022\n",
      " 0.99712746 0.         0.99353292 0.99712746 0.93462778 0.91952503\n",
      " 0.99712746 0.9619066  0.98132591 0.92456615 0.93677316 0.93103582\n",
      " 0.99712746 0.91523942 1.                nan 0.99856373 0.92959697\n",
      " 0.99640803 0.92240788 0.99856373        nan 0.99712746 0.93175266\n",
      " 0.87430701 0.99784688 0.91305794 0.94970475 0.93606921 0.\n",
      " 0.99712746 0.93031381 0.9446791  0.9719837  0.94827106 0.33046079\n",
      " 0.99712746 0.99640803 0.97055001 0.98491014 0.91165003 0.96982285\n",
      "        nan        nan 0.         0.83552513 0.92096903        nan\n",
      " 0.93606921 0.         0.86999046 0.         0.92241304 0.93462778\n",
      " 0.99784431 0.99712746 0.99712746 0.90517264 0.99856115 0.50715299\n",
      " 0.99712746 0.99712746 0.99712746 0.926005   0.92959697 0.99784688\n",
      " 0.91665506 0.94395194 0.94540368 0.91093319 0.95904953 0.92455842\n",
      " 0.87287331 0.99712746 0.99712746 0.95116423 0.94611794 1.\n",
      " 0.90014698 0.92240788 0.9353601  1.         0.93247209 0.99712746\n",
      " 0.99928315        nan 0.         0.99712746 0.99712746 0.94970475\n",
      " 0.99712746 0.99712746 0.95617184 0.94037544 0.92528558 0.92959697\n",
      " 0.99712746 0.99856115 0.99712746 0.92528042 0.99784688 0.93031381\n",
      " 0.99784431 0.95760553 0.99784431 0.99928058 0.95617442 0.99712746\n",
      "        nan 0.89726155 0.99784688 0.93893917 0.99712746 0.93893917\n",
      " 0.92959697 0.99640803 0.99712746 0.94179882 0.92887754 0.99712746\n",
      " 0.9921018  0.         0.92169877 0.99712746 0.99712746 0.99712746\n",
      " 0.92455842 0.94180398 0.99640803 0.89726155 0.92959697 0.93318893\n",
      " 0.92096903 0.98489982 0.96910601 0.99784431 0.92455842 0.99712746\n",
      " 0.94611794        nan 0.99712746 0.92958407 0.92455842 0.98490756\n",
      " 0.93534463        nan 0.         0.92672443 0.99784431 0.93463036\n",
      " 0.9238622  0.86423248 0.80387561 0.99712746 0.92455842 0.71406617\n",
      "        nan 0.95832753 0.99640803 0.95617184 0.87430701 0.99856373\n",
      " 0.92096903 0.9231273  0.99712746 0.97269797        nan 0.98994095\n",
      " 0.97485882 0.         0.92169877 0.92240788 0.78808953 0.91450453\n",
      " 0.92672443 0.92455842 0.30744437 0.92455842 0.89942498        nan\n",
      " 0.93893144 0.925283   0.53450143 0.91307083 0.94970991 0.94755679\n",
      " 0.96048323 0.         0.99496919 0.97844305 0.         0.99928058\n",
      " 0.99784431 0.99712746 0.86927361 0.                nan 0.97341482\n",
      " 0.9375029         nan 0.95832495 0.92097935 0.99712746 0.99712746\n",
      " 0.93821201 0.         0.99928058 0.99712746 0.99712746 0.92385446\n",
      " 0.87285784 0.93391094 0.9712617  0.99641061 0.96407777 0.92959697\n",
      " 0.99712746 0.98275444 0.98922153 0.92240788 0.86782445 0.98347902\n",
      " 0.97198112 0.92455842 0.99928058 0.99712746 0.99712746 0.9870942\n",
      " 0.99712746 0.99784431 0.90301952 0.92959697 0.99784688 0.99712746\n",
      " 0.97773136 0.94682963 0.93031381        nan 0.99712746 0.99712746\n",
      " 0.92455842 0.90877749 0.92959697 0.96623862 0.97486656 0.94108971\n",
      "        nan        nan 0.92816328 0.95545499 0.894389   0.88651144\n",
      " 0.97270313 0.92959697 0.92455842 0.99209922 0.99712746 0.99712746\n",
      "        nan 0.97772878        nan 0.90661149 0.91809133 0.93821201\n",
      " 1.         0.94396483 0.9181068  0.91809907 0.96120007 0.99712746\n",
      " 0.96048323 0.99928058 0.94970733 0.92096903 0.92455842 0.96982801\n",
      " 0.92815812 0.92455842 0.99712746 0.47842758 0.93463552 0.92096903\n",
      " 0.99712746 0.92240788 0.99928058 0.50938346 0.99425492 0.96335061\n",
      " 0.92887754 0.78879864 0.94683479 0.99713004 0.99712746 0.99784431\n",
      " 0.99856373 0.96695031 0.99281607 0.99712746 0.92455842 0.99784688\n",
      " 0.99712746 0.99712746 0.93606147 1.         0.92096903 0.99712746\n",
      " 0.99712746 0.99712746 0.99712746 0.89079704 0.54097625 0.99712746\n",
      " 0.94611279 0.99928058 0.87430701 0.92959697 0.99712746 0.81831567\n",
      " 0.91954308 0.92384931 0.92240788 0.92455842 0.92815812 0.99856373\n",
      " 0.95617184 0.92240788 0.88936335 0.92959697 0.99712746 0.99712746\n",
      " 0.92456615 0.         0.99784688 0.93390578 0.93535236 0.99784431\n",
      " 0.92959697 0.93821459 0.99425492 0.83983394        nan 0.92959697\n",
      " 0.99712746 0.90660633 0.92096903 0.99712746 0.92455842        nan\n",
      " 0.97629251        nan 0.97629251 0.92455842 0.93893401 0.82330007\n",
      " 0.92959697 0.87429411 0.95832753 0.92455842 0.         0.50715299\n",
      " 0.97059643 1.         0.93462778 0.99712746 0.89079188 0.94611279\n",
      " 0.99209407 0.92383384 0.92385189 0.9331812  0.93319667        nan\n",
      " 0.92097935 0.93606921 0.89079188 0.99712746 1.         0.99712746\n",
      " 0.99209922 0.97845594 0.92169877 0.         0.95402388 0.99784431\n",
      " 0.9087646  0.9820376  0.         0.99712746 0.9066218         nan\n",
      " 0.92816328 0.97700936 0.99569635        nan 0.95044223 0.99784688\n",
      " 0.92959697 0.89726155 0.99856373 0.99928058        nan 0.92385189\n",
      " 0.92959697 0.99712746 0.99784688 0.96551404 0.96192981 0.99784688\n",
      " 0.93821975 0.81979578 0.92672443 0.99712746 0.95544983 0.99353549\n",
      " 0.92240788 0.90661149]\n",
      "  warnings.warn(\n",
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.9655486542443065\n",
      "... with parameters: {'n_estimators': 2000, 'min_samples_split': 19, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0071, 'max_depth': 2, 'loss': 'deviance', 'learning_rate': 0.2, 'criterion': 'friedman_mse'}\n"
     ]
    }
   ],
   "source": [
    "#Grid for Gradient Boost Classifier\n",
    "\n",
    "param_grid_gb = {  \n",
    "    'min_samples_split': np.arange(1,20),  \n",
    "    'min_samples_leaf': np.arange(1,12),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005), \n",
    "    'loss': ['log_loss', 'deviance', 'exponential'],\n",
    "    'criterion': ['friedman_mse', 'squared_error'],\n",
    "    'max_depth': range (2, 10, 1),\n",
    "    'n_estimators': [10,50,250,1000,2000],\n",
    "    'learning_rate': [1.0,0.2,0.1, 0.01, 0.05],}   \n",
    "\n",
    "rand_search_gb = RandomizedSearchCV(estimator = gboost, param_distributions=param_grid_gb, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Gradient Boost Classifier model fit for grid search\n",
    "_ = rand_search_gb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_gb.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_gb.best_params_}\")\n",
    "\n",
    "bestRecallgb = rand_search_gb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'adam', 'max_iter': 5000, 'learning_rate_init': 0.001, 'learning_rate': 'invscaling', 'hidden_layer_sizes': (50,), 'alpha': 0, 'activation': 'logistic'}\n",
      "CPU times: total: 2.81 s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid_nn = {\n",
    "    'hidden_layer_sizes': [ (50,), (70,), (100,), (50,30), (40,20), (50,30, 20), (70,50,40)],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0, .2, .5, .7, 1],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1, 0.2, 0.5],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "\n",
    "rand_search_nn = RandomizedSearchCV(estimator = ann, param_distributions=param_grid_nn, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search_nn.fit(X_train, y_train)\n",
    "\n",
    "bestRecallNn = rand_search_nn.best_estimator_\n",
    "\n",
    "print(rand_search_nn.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 5ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "{'optimizer__learning_rate': 0.0005, 'optimizer': 'sgd', 'model__hidden_layer_sizes': (100, 90), 'model__dropout': 0, 'epochs': 100, 'batch_size': 20}\n",
      "CPU times: total: 7min 40s\n",
      "Wall time: 24min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = keras.models.Sequential()\n",
    "recall_metric = tf.keras.metrics.Recall()\n",
    "def build_clf(hidden_layer_sizes, dropout):\n",
    "    ann = tf.keras.models.Sequential()\n",
    "    ann.add(keras.layers.Input(shape=36)),\n",
    "    for hidden_layer_size in hidden_layer_sizes:\n",
    "        model.add(keras.layers.Dense(hidden_layer_size, kernel_initializer= tf.keras.initializers.GlorotNormal(seed=1), \n",
    "                                     bias_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation=\"relu\"))\n",
    "        model.add(keras.layers.Dropout(dropout))\n",
    "    ann.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    ann.compile(loss = 'binary_crossentropy', metrics = [recall_metric])\n",
    "    return ann\n",
    "\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "keras_clf = KerasClassifier(\n",
    "    model=build_clf,\n",
    "    hidden_layer_sizes=36,\n",
    "    dropout = 0.0\n",
    ")\n",
    "\n",
    "\n",
    "params = {\n",
    "    'optimizer__learning_rate': [0.0005, 0.001, 0.005],\n",
    "    'model__hidden_layer_sizes': [(90, ), (100,), (100, 90), (100,90,70)],\n",
    "    'model__dropout': [0, 0.1],\n",
    "    'batch_size':[20, 60, 100],\n",
    "    'epochs':[10, 50, 100],\n",
    "    'optimizer':[\"adam\",'sgd']\n",
    "}\n",
    "keras_clf.get_params().keys()\n",
    "\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(estimator=keras_clf, param_distributions=params, scoring='recall', n_iter=50, cv=5)\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(10000) # note: the default is 3000 (python 3.9)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "callback = [earlystop]\n",
    "\n",
    "_ = rnd_search_cv.fit(X_train, y_train, callbacks=callback, verbose=0)\n",
    "\n",
    "rnd_search_cv.best_params_\n",
    "\n",
    "best_net = rnd_search_cv.best_estimator_\n",
    "print(rnd_search_cv.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================LOGISTIC REGRESSION====================\n",
      "\n",
      "The best recall score is 0.9539958592132505\n",
      "... with parameters: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 306}\n",
      "\n",
      "=========================DECISION TREE==========================\n",
      "\n",
      "The best recall score is 0.9453416149068324\n",
      "... with parameters: {'min_samples_split': 31, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.0021, 'max_leaf_nodes': 43, 'max_depth': 15, 'criterion': 'entropy'}\n",
      "\n",
      "==============================SVM===============================\n",
      "\n",
      "The best recall score is 0.9712629399585921\n",
      "... with parameters: {'kernel': 'linear', 'C': 1}\n",
      "\n",
      "=========================ADABOOST===============================\n",
      "\n",
      "The best recall score is 0.959792960662526\n",
      "... with parameters: {'n_estimators': 1000, 'learning_rate': 0.1}\n",
      "\n",
      "=========================RANDOMFOREST===========================\n",
      "\n",
      "The best recall score is 0.959792960662526\n",
      "... with parameters: {'n_estimators': 250, 'max_features': 'auto', 'max_depth': 10, 'criterion': 'entropy'}\n",
      "\n",
      "=========================XGBOOST================================\n",
      "\n",
      "The best recall score is 0.9655072463768116\n",
      "... with parameters: {'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.2}\n",
      "\n",
      "=========================GRADIENT BOOST================================\n",
      "\n",
      "The best recall score is 0.9655486542443065\n",
      "... with parameters: {'n_estimators': 2000, 'min_samples_split': 19, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0071, 'max_depth': 2, 'loss': 'deviance', 'learning_rate': 0.2, 'criterion': 'friedman_mse'}\n",
      "\n",
      "=========================NEURAL NETWORK================================\n",
      "\n",
      "The best recall score is 0.913623188405797\n",
      "... with parameters: {'solver': 'adam', 'max_iter': 5000, 'learning_rate_init': 0.001, 'learning_rate': 'invscaling', 'hidden_layer_sizes': (50,), 'alpha': 0, 'activation': 'logistic'}\n",
      "\n",
      "=========================DEEP NEURAL NETWORK================================\n",
      "\n",
      "The best recall score is 0.9105590062111801\n",
      "... with parameters: {'optimizer__learning_rate': 0.0005, 'optimizer': 'sgd', 'model__hidden_layer_sizes': (100, 90), 'model__dropout': 0, 'epochs': 100, 'batch_size': 20}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=========================LOGISTIC REGRESSION====================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_logr.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_logr.best_params_}\")\n",
    "print(\"\\n=========================DECISION TREE==========================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_tree.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_tree.best_params_}\")\n",
    "print(\"\\n==============================SVM===============================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_svm.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_svm.best_params_}\")\n",
    "print(\"\\n=========================ADABOOST===============================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_ada.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_ada.best_params_}\")\n",
    "print(\"\\n=========================RANDOMFOREST===========================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_rf.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_rf.best_params_}\")\n",
    "print(\"\\n=========================XGBOOST================================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_xg.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_xg.best_params_}\")\n",
    "print(\"\\n=========================GRADIENT BOOST================================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_gb.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_gb.best_params_}\")\n",
    "print(\"\\n=========================NEURAL NETWORK================================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_nn.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_nn.best_params_}\")\n",
    "print(\"\\n=========================DEEP NEURAL NETWORK================================\\n\")\n",
    "print(f\"The best {score_measure} score is {rnd_search_cv.best_score_}\")\n",
    "print(f\"... with parameters: {rnd_search_cv.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Grid Search ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.959792960662526\n",
      "... with parameters: {'learning_rate': 0.07, 'n_estimators': 800}\n"
     ]
    }
   ],
   "source": [
    "# ADABoosting classifier grid\n",
    "param_grid_ada = {  \n",
    "     'n_estimators': [800,1000,1200],\n",
    "     'learning_rate': [0.07,0.1,0.13],}  \n",
    "\n",
    "\n",
    "grid_search_ada = GridSearchCV(estimator = adatree, param_grid=param_grid_ada, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# ADABoosting classifier fit\n",
    "_ = grid_search_ada.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_ada.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_ada.best_params_}\")\n",
    "\n",
    "bestRecallAda = grid_search_ada.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "The best recall score is 0.9655072463768116\n",
      "... with parameters: {'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "#Grid for XGBoost Classifier\n",
    "\n",
    "param_grid_xg = {  \n",
    "    'max_depth': [2,4,6],\n",
    "    'n_estimators': [200,250,300],\n",
    "    'learning_rate': [0.17,0.2,0.23],}   \n",
    "\n",
    "grid_search_xg = GridSearchCV(estimator = xgboost, param_grid=param_grid_xg, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# XGBoost Classifier model fit for grid search\n",
    "_ = grid_search_xg.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_xg.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_xg.best_params_}\")\n",
    "\n",
    "bestRecallXg = grid_search_xg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\model_selection\\_search.py:909: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.9569358178053831\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 40}\n"
     ]
    }
   ],
   "source": [
    "#Grid for Randomforest Classifier\n",
    "param_grid_rf = {  \n",
    "     'n_estimators': [40,50,60],\n",
    "     'max_features': ['sqrt'],\n",
    "     'max_depth' : [8,10,12],\n",
    "     'criterion' :['entropy'],}   \n",
    "\n",
    "grid_search_rf = GridSearchCV(estimator = rforest, param_grid=param_grid_rf, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# ADABoost Classifier model fit for grid search\n",
    "_ = grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_rf.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_rf.best_params_}\")\n",
    "\n",
    "bestRecallRf = grid_search_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.9712629399585921\n",
      "... with parameters: {'C': 1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Grid for SVM\n",
    "param_grid_svm = {\n",
    "    'C': [1,2,3],\n",
    "    'kernel': ['linear'],  \n",
    "}\n",
    "\n",
    "grid_search_svm = GridSearchCV(estimator = svmm, param_grid=param_grid_svm, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# SVM model fit for grid search\n",
    "_ = grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_svm.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_svm.best_params_}\")\n",
    "\n",
    "bestRecallSvm = grid_search_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "The best recall score is 0.94824016563147\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 13, 'max_leaf_nodes': 41, 'min_impurity_decrease': 0.0018, 'min_samples_leaf': 2, 'min_samples_split': 29}\n"
     ]
    }
   ],
   "source": [
    "# Grid for decision tree\n",
    " \n",
    "param_grid_tree = {\n",
    "    'min_samples_split': [29,31,33],  \n",
    "    'min_samples_leaf': [2,4,6],\n",
    "    'min_impurity_decrease': [0.0018,0.0021,0.0024],\n",
    "    'max_leaf_nodes': [41,43,45], \n",
    "    'max_depth': [13,15,17], \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "grid_search_tree = GridSearchCV(estimator = dtree, param_grid=param_grid_tree, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Decision tree model fit for grid search\n",
    "_ = grid_search_tree.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_tree.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_tree.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "The best recall score is 0.9539958592132505\n",
      "... with parameters: {'max_iter': 280, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Grid for Logistic Regression\n",
    "\n",
    "param_grid_logr = {\n",
    "     'penalty': ['l1'],\n",
    "     'solver': ['liblinear'],\n",
    "     'max_iter': [280,306,330],\n",
    "}\n",
    "\n",
    "grid_search_logr = GridSearchCV(estimator = logreg, param_grid=param_grid_logr, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Logistic Regression model fit for grid search\n",
    "_ = grid_search_logr.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_logr.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_logr.best_params_}\")\n",
    "\n",
    "bestRecallLogr = grid_search_logr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.9626501035196687\n",
      "... with parameters: {'criterion': 'friedman_mse', 'learning_rate': 0.07, 'loss': 'deviance', 'max_depth': 6, 'min_impurity_decrease': 0.0059, 'min_samples_leaf': 8, 'min_samples_split': 17, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "#Grid for Gradient Boost Classifier\n",
    "\n",
    "param_grid_gb = {  \n",
    "    'min_samples_split': [17,19,21],  \n",
    "    'min_samples_leaf': [6,8,10],\n",
    "    'min_impurity_decrease': [0.0059,0.0061,0.0063], \n",
    "    'loss': ['deviance'],\n",
    "    'criterion': ['friedman_mse'],\n",
    "    'max_depth': [4,6,8],\n",
    "    'n_estimators': [200,250,300],\n",
    "    'learning_rate': [0.07,0.1,0.13],}   \n",
    "\n",
    "grid_search_gb = GridSearchCV(estimator = gboost, param_grid=param_grid_gb, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Gradient Boost Classifier model fit for grid search\n",
    "_ = grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_gb.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_gb.best_params_}\")\n",
    "\n",
    "bestRecallgb = rand_search_gb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'logistic', 'alpha': 0, 'hidden_layer_sizes': (50,), 'learning_rate': 'invscaling', 'learning_rate_init': 0.0007, 'max_iter': 4500, 'solver': 'adam'}\n",
      "CPU times: total: 3.86 s\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    " \n",
    "param_grid_nn = {\n",
    "    'hidden_layer_sizes': [ (40,), (50,), (60,)],\n",
    "    'activation': ['logistic'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0,.3, .5, .7],\n",
    "    'learning_rate': ['invscaling'],\n",
    "    'learning_rate_init': [0.0007, 0.001, 0.0013 ],\n",
    "    'max_iter': [4500,5000,5500]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search_nn = GridSearchCV(estimator = ann, param_grid=param_grid_nn, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search_nn.fit(X_train, y_train)\n",
    "\n",
    "bestRecallNn = grid_search_nn.best_estimator_\n",
    "\n",
    "print(grid_search_nn.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 920us/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 794us/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 5ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 1s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 5ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 4ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 3ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 3ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 3ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "11/11 [==============================] - 0s 2ms/step\n",
      "{'batch_size': 10, 'epochs': 100, 'model__dropout': 0.1, 'model__hidden_layer_sizes': (100, 90), 'optimizer': 'sgd', 'optimizer__learning_rate': 0.0005}\n",
      "CPU times: total: 8h 45min 46s\n",
      "Wall time: 4h 33min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    " \n",
    "model = keras.models.Sequential()\n",
    "recall_metric = tf.keras.metrics.Recall()\n",
    "def build_clf(hidden_layer_sizes, dropout):\n",
    "    ann = tf.keras.models.Sequential()\n",
    "    ann.add(keras.layers.Input(shape=36)),\n",
    "    for hidden_layer_size in hidden_layer_sizes:\n",
    "        model.add(keras.layers.Dense(hidden_layer_size, kernel_initializer= tf.keras.initializers.GlorotNormal(seed=1), \n",
    "                                     bias_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation=\"relu\"))\n",
    "        model.add(keras.layers.Dropout(dropout))\n",
    "    ann.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    ann.compile(loss = 'binary_crossentropy', metrics = [recall_metric])\n",
    "    return ann\n",
    "\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "keras_clf = KerasClassifier(\n",
    "    model=build_clf,\n",
    "    hidden_layer_sizes=36,\n",
    "    dropout = 0.0\n",
    ")\n",
    "\n",
    "\n",
    "params = {\n",
    "    'optimizer__learning_rate': [0.0002,0.0005, 0.0008],\n",
    "    'model__hidden_layer_sizes': [(100,70), (100, 90), (120,90)],\n",
    "    'model__dropout': [0, 0.1],\n",
    "    'batch_size':[10, 20, 30],\n",
    "    'epochs':[80,100,120],\n",
    "    'optimizer':['sgd']\n",
    "}\n",
    "keras_clf.get_params().keys()\n",
    "\n",
    "\n",
    "grid_search_cv = GridSearchCV(estimator=keras_clf, param_grid=params, scoring='recall', cv=5)\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(10000) # note: the default is 3000 (python 3.9)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "callback = [earlystop]\n",
    "\n",
    "_ = grid_search_cv.fit(X_train, y_train, callbacks=callback, verbose=0)\n",
    "\n",
    "grid_search_cv.best_params_\n",
    "\n",
    "bestRecallnet = grid_search_cv.best_estimator_\n",
    "print(grid_search_cv.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final models with best parameters ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = bestRecallTree\n",
    "svmm = bestRecallSvm  \n",
    "logreg = bestRecallLogr\n",
    "adatree = bestRecallAda\n",
    "rforest = bestRecallRf\n",
    "xgboost = bestRecallXg\n",
    "gboost = bestRecallgb\n",
    "ann = bestRecallNn\n",
    "dnn = bestRecallnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fit for train dataset and prediction with test dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.959349593495935\n",
      "Accuracy Score:   0.9887218045112782\n",
      "Precision Score:  0.9915966386554622\n",
      "F1 Score:         0.9752066115702479\n"
     ]
    }
   ],
   "source": [
    "_ = xgboost.fit(X_train, y_train)\n",
    "y_pred = xgboost.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "xgboost_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\AppData\\Local\\Temp\\ipykernel_31804\\214086440.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  _ = rforest.fit(X_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.975609756097561\n",
      "Accuracy Score:   0.9924812030075187\n",
      "Precision Score:  0.9917355371900827\n",
      "F1 Score:         0.9836065573770492\n"
     ]
    }
   ],
   "source": [
    "_ = rforest.fit(X_train, y_train)\n",
    "y_pred = rforest.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "rforest_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.983739837398374\n",
      "Accuracy Score:   0.9962406015037594\n",
      "Precision Score:  1.0\n",
      "F1 Score:         0.9918032786885246\n"
     ]
    }
   ],
   "source": [
    "_ = adatree.fit(X_train, y_train)\n",
    "y_pred = adatree.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "adatree_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.975609756097561\n",
      "Accuracy Score:   0.9906015037593985\n",
      "Precision Score:  0.9836065573770492\n",
      "F1 Score:         0.9795918367346939\n"
     ]
    }
   ],
   "source": [
    "_ = logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "logreg_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.975609756097561\n",
      "Accuracy Score:   0.9943609022556391\n",
      "Precision Score:  1.0\n",
      "F1 Score:         0.9876543209876543\n"
     ]
    }
   ],
   "source": [
    "_ = svmm.fit(X_train, y_train)\n",
    "y_pred = svmm.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "svmm_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.967479674796748\n",
      "Accuracy Score:   0.9830827067669173\n",
      "Precision Score:  0.9596774193548387\n",
      "F1 Score:         0.9635627530364373\n"
     ]
    }
   ],
   "source": [
    "_ = dtree.fit(X_train, y_train)\n",
    "y_pred = dtree.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "dtree_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.983739837398374\n",
      "Accuracy Score:   0.9962406015037594\n",
      "Precision Score:  1.0\n",
      "F1 Score:         0.9918032786885246\n"
     ]
    }
   ],
   "source": [
    "_ = gboost.fit(X_train, y_train)\n",
    "y_pred = gboost.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "gboost_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.959349593495935\n",
      "Accuracy Score:   0.9887218045112782\n",
      "Precision Score:  0.9915966386554622\n",
      "F1 Score:         0.9752066115702479\n"
     ]
    }
   ],
   "source": [
    "_ = ann.fit(X_train, y_train)\n",
    "y_pred = ann.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "ann_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "160/160 [==============================] - 1s 2ms/step - loss: 36.9335 - recall_1: 0.6667\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 18.8301 - recall_1: 0.4080\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 7.4310 - recall_1: 0.4195\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 4.9187 - recall_1: 0.4483\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 3.8080 - recall_1: 0.5144\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 3.0413 - recall_1: 0.5230\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 2.4634 - recall_1: 0.5747\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 2.0708 - recall_1: 0.6092\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.7404 - recall_1: 0.5920\n",
      "Epoch 10/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.5156 - recall_1: 0.6408\n",
      "Epoch 11/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.3473 - recall_1: 0.6379\n",
      "Epoch 12/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.2193 - recall_1: 0.6753\n",
      "Epoch 13/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0928 - recall_1: 0.6695\n",
      "Epoch 14/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 1.0220 - recall_1: 0.6667\n",
      "Epoch 15/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9267 - recall_1: 0.6810\n",
      "Epoch 16/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8503 - recall_1: 0.7011\n",
      "Epoch 17/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7925 - recall_1: 0.7126\n",
      "Epoch 18/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7342 - recall_1: 0.7155\n",
      "Epoch 19/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6875 - recall_1: 0.7126\n",
      "Epoch 20/100\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.6236 - recall_1: 0.7414\n",
      "Epoch 21/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5802 - recall_1: 0.7385\n",
      "Epoch 22/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.5621 - recall_1: 0.7356\n",
      "Epoch 23/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.5263 - recall_1: 0.7241\n",
      "Epoch 24/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.4822 - recall_1: 0.7328\n",
      "Epoch 25/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.4950 - recall_1: 0.7356\n",
      "Epoch 26/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.4441 - recall_1: 0.7500\n",
      "Epoch 27/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.4376 - recall_1: 0.7557\n",
      "Epoch 28/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.4305 - recall_1: 0.7701\n",
      "Epoch 29/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.3885 - recall_1: 0.7471\n",
      "Epoch 30/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.3983 - recall_1: 0.7557\n",
      "Epoch 31/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.3603 - recall_1: 0.7701\n",
      "Epoch 32/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.3717 - recall_1: 0.7701\n",
      "Epoch 33/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.3645 - recall_1: 0.7672\n",
      "Epoch 34/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.3065 - recall_1: 0.7701\n",
      "Epoch 35/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.3048 - recall_1: 0.7845\n",
      "Epoch 36/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.3044 - recall_1: 0.7816\n",
      "Epoch 37/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.2811 - recall_1: 0.7960\n",
      "Epoch 38/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.2741 - recall_1: 0.8075\n",
      "Epoch 39/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.2917 - recall_1: 0.7989\n",
      "Epoch 40/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.2634 - recall_1: 0.8046\n",
      "Epoch 41/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.2590 - recall_1: 0.8190\n",
      "Epoch 42/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.2572 - recall_1: 0.8247\n",
      "Epoch 43/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.2295 - recall_1: 0.8420\n",
      "Epoch 44/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.2474 - recall_1: 0.8362\n",
      "Epoch 45/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.2210 - recall_1: 0.8448\n",
      "Epoch 46/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.2347 - recall_1: 0.8305\n",
      "Epoch 47/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.2165 - recall_1: 0.8534\n",
      "Epoch 48/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.2096 - recall_1: 0.8391\n",
      "Epoch 49/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.2045 - recall_1: 0.8391\n",
      "Epoch 50/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.2038 - recall_1: 0.8305\n",
      "Epoch 51/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.2226 - recall_1: 0.8563\n",
      "Epoch 52/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.2015 - recall_1: 0.8649\n",
      "Epoch 53/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.1696 - recall_1: 0.8678\n",
      "Epoch 54/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.1884 - recall_1: 0.8420\n",
      "Epoch 55/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1832 - recall_1: 0.8621\n",
      "Epoch 56/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1770 - recall_1: 0.8649\n",
      "Epoch 57/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1800 - recall_1: 0.8621\n",
      "Epoch 58/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1638 - recall_1: 0.8649\n",
      "Epoch 59/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1816 - recall_1: 0.8678\n",
      "Epoch 60/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1658 - recall_1: 0.8793\n",
      "Epoch 61/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1706 - recall_1: 0.8649\n",
      "Epoch 62/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1656 - recall_1: 0.8534\n",
      "Epoch 63/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1447 - recall_1: 0.8879\n",
      "Epoch 64/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1562 - recall_1: 0.8793\n",
      "Epoch 65/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1596 - recall_1: 0.8678\n",
      "Epoch 66/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.1556 - recall_1: 0.8851\n",
      "Epoch 67/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.1543 - recall_1: 0.8822\n",
      "Epoch 68/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.1444 - recall_1: 0.8851\n",
      "Epoch 69/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.1350 - recall_1: 0.8966\n",
      "Epoch 70/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1595 - recall_1: 0.8966\n",
      "Epoch 71/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1350 - recall_1: 0.8822\n",
      "Epoch 72/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1431 - recall_1: 0.8793\n",
      "Epoch 73/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1354 - recall_1: 0.8994\n",
      "Epoch 74/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.1315 - recall_1: 0.9138\n",
      "Epoch 75/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.1245 - recall_1: 0.9080\n",
      "Epoch 76/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.1270 - recall_1: 0.8994\n",
      "Epoch 77/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.1359 - recall_1: 0.8764\n",
      "Epoch 78/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1210 - recall_1: 0.8937\n",
      "Epoch 79/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1273 - recall_1: 0.8822\n",
      "Epoch 80/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1250 - recall_1: 0.8994\n",
      "Epoch 81/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1334 - recall_1: 0.8851\n",
      "Epoch 82/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1336 - recall_1: 0.8908\n",
      "Epoch 83/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1339 - recall_1: 0.9195\n",
      "Epoch 84/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1164 - recall_1: 0.9080\n",
      "Epoch 85/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1209 - recall_1: 0.8994\n",
      "Epoch 86/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1366 - recall_1: 0.9023\n",
      "Epoch 87/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1228 - recall_1: 0.9080\n",
      "Epoch 88/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.1240 - recall_1: 0.9195\n",
      "Epoch 89/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.1202 - recall_1: 0.8994\n",
      "Epoch 90/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.1076 - recall_1: 0.9138\n",
      "Epoch 91/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.1124 - recall_1: 0.9023\n",
      "Epoch 92/100\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.1169 - recall_1: 0.9138\n",
      "Epoch 93/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1222 - recall_1: 0.9253\n",
      "Epoch 94/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1079 - recall_1: 0.9195\n",
      "Epoch 95/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1247 - recall_1: 0.9195\n",
      "Epoch 96/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1113 - recall_1: 0.9109\n",
      "Epoch 97/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1080 - recall_1: 0.9080\n",
      "Epoch 98/100\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 0.0931 - recall_1: 0.9138\n",
      "Epoch 99/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1281 - recall_1: 0.9109\n",
      "Epoch 100/100\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.1015 - recall_1: 0.9253\n",
      "54/54 [==============================] - 0s 2ms/step\n",
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.967479674796748\n",
      "Accuracy Score:   0.981203007518797\n",
      "Precision Score:  0.952\n",
      "F1 Score:         0.9596774193548387\n"
     ]
    }
   ],
   "source": [
    "_ = dnn.fit(X_train, y_train)\n",
    "y_pred = dnn.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "dnn_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall scores...\n",
      "Decision Tree:           0.967479674796748\n",
      "SVM:                     0.975609756097561\n",
      "Logistic Regression      0.975609756097561\n",
      "\n",
      "===========================================\n",
      "\n",
      "Random Forest:           0.975609756097561\n",
      "Ada Boosted Tree:        0.983739837398374\n",
      "XGBoost Tree:            0.959349593495935\n",
      "Gradient Boosting        0.983739837398374\n",
      "\n",
      "===========================================\n",
      "\n",
      "Neural Network           0.959349593495935\n",
      "Deep Neural Network      0.967479674796748\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall scores...\")\n",
    "print(f\"{'Decision Tree:':25}{dtree_recall}\")\n",
    "print(f\"{'SVM:':25}{svmm_recall}\")\n",
    "print(f\"{'Logistic Regression':25}{logreg_recall}\")\n",
    "print(\"\\n===========================================\\n\")\n",
    "print(f\"{'Random Forest:':25}{rforest_recall}\")\n",
    "print(f\"{'Ada Boosted Tree:':25}{adatree_recall}\")\n",
    "print(f\"{'XGBoost Tree:':25}{xgboost_recall}\")\n",
    "print(f\"{'Gradient Boosting':25}{gboost_recall}\")\n",
    "print(\"\\n===========================================\\n\")\n",
    "print(f\"{'Neural Network':25}{ann_recall}\")\n",
    "print(f\"{'Deep Neural Network':25}{dnn_recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Models ##\n",
    "\n",
    "The analysis of recall score of all the models shows that the noural network doesnot perform better than ensemble models. The neural network scores 0.96 and deep neural network scores 0.97 which are same as decision tree and random forest but the gradient bossting model and Ada Boost are the best with recall score of 0.98."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model to disk ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save model\n",
    "pickle.dump(gboost, open('./gboost_model.pkl', \"wb\"))\n",
    "\n",
    "# If you wish to load this model later, simply use pickle.load method\n",
    "#loaded_model = pickle.load(open('logistic_model_example01.pkl', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Class08b-decision_tree_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b056086e24cb5602cbcb82122035cd3d6ee2ccbf5df29c16e348c108b0f83be3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
