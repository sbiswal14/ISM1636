{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxGJbLsUhuc8"
   },
   "source": [
    "## Subash Chandra Biswal (U77884251) ##\n",
    "# Assignment 1 - Cardiotocography\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tuXRZKEYrDa"
   },
   "source": [
    "## Introduction and Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q08EVUytY3eh"
   },
   "source": [
    "Author: J. P. Marques de SÃ¡, J. Bernardes, D. Ayers de Campos.  \n",
    "Source: UCI  \n",
    "Please cite: Ayres de Campos et al. (2000) SisPorto 2.0 A Program for Automated Analysis of Cardiotocograms. J Matern Fetal Med 5:311-318, UCI    \n",
    "\n",
    "2126 fetal cardiotocograms (CTGs) were automatically processed and the respective diagnostic features measured. The CTGs were also classified by three expert obstetricians and a consensus classification label assigned to each of them. Classification was both with respect to a morphologic pattern (A, B, C. ...) and to a fetal state (N, S, P). Therefore the dataset can be used either for 10-class or 3-class experiments.  \n",
    "\n",
    "Attribute Information:  \n",
    "LB - FHR baseline (beats per minute)  \n",
    "AC - # of accelerations per second  \n",
    "FM - # of fetal movements per second  \n",
    "UC - # of uterine contractions per second  \n",
    "DL - # of light decelerations per second  \n",
    "DS - # of severe decelerations per second  \n",
    "DP - # of prolongued decelerations per second  \n",
    "ASTV - percentage of time with abnormal short term variability  \n",
    "MSTV - mean value of short term variability  \n",
    "ALTV - percentage of time with abnormal long term variability  \n",
    "MLTV - mean value of long term variability  \n",
    "Width - width of FHR histogram  \n",
    "Min - minimum of FHR histogram  \n",
    "Max - Maximum of FHR histogram  \n",
    "Nmax - # of histogram peaks  \n",
    "Nzeros - # of histogram zeros  \n",
    "Mode - histogram mode  \n",
    "Mean - histogram mean  \n",
    "Median - histogram median  \n",
    "Variance - histogram variance  \n",
    "Tendency - histogram tendency  \n",
    "CLASS - FHR pattern class code (1 to 10)  \n",
    "NSP - fetal state class code (N=normal(1); S=suspect(2); P=pathologic(3))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmYLcm3aY8X5"
   },
   "source": [
    "## Install and import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8zNdljvIhuc8"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "# set random seed to ensure that results are repeatable\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGgrXNQPZT3J"
   },
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "q3u5LsGyhudA"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"./X_train.csv\")\n",
    "y_train = pd.read_csv(\"./y_train.csv\")\n",
    "X_test = pd.read_csv(\"./X_test.csv\")\n",
    "y_test = pd.read_csv(\"./y_test.csv\")\n",
    "X = pd.read_csv(\"./X.csv\")\n",
    "y = pd.read_csv(\"./y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics ##\n",
    "Since this is pharmacutical data and we are targeting suspects from various medical test data, we need to minimize the false negatives as this will cost somebody's life. This cost is significantly high as compared to false positive and in case of false positive the patient/insurance company needs to bear only the further investigation costs. \n",
    "\n",
    "Since this is a classification problem our score metrics is confusion matix and our measure of score is recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "svmm = SVC()\n",
    "logreg = LogisticRegression()\n",
    "adatree = AdaBoostClassifier()\n",
    "rforest = RandomForestClassifier()\n",
    "xgboost = XGBClassifier()\n",
    "gboost = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Random search of parameter grids of all models ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "The best recall score is 0.9539958592132505\n",
      "... with parameters: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 306}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "70 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.95399586 0.61486542 0.95395445 0.54302277        nan 0.62062112\n",
      "        nan 0.76132505 0.89358178 0.64078675 0.41097308 0.87639752\n",
      " 0.38501035        nan 0.89072464 0.40517598 0.95395445 0.7699793\n",
      " 0.95395445 0.95399586 0.8447205  0.95399586 0.82173913 0.95399586\n",
      " 0.61486542 0.61486542 0.7010766  0.78149068 0.69254658 0.95399586\n",
      " 0.7010766  0.63213251 0.89358178 0.95399586 0.95395445 0.95395445\n",
      " 0.61776398 0.88215321        nan 0.62637681 0.63213251 0.95395445\n",
      " 0.78140787 0.46277433 0.95399586 0.63213251        nan 0.63213251\n",
      "        nan 0.95399586 0.5257971  0.95395445 0.59763975 0.95395445\n",
      " 0.95395445 0.63213251        nan        nan 0.50567288 0.95399586\n",
      " 0.95395445 0.51428571 0.63792961 0.95395445 0.62637681 0.82459627\n",
      " 0.62923395 0.41966874 0.57461698        nan 0.62637681 0.86484472\n",
      " 0.72405797 0.74993789 0.52869565 0.64078675 0.63503106 0.39937888\n",
      " 0.82178054        nan 0.7642236  0.95399586        nan 0.63213251\n",
      "        nan 0.95399586 0.49995859 0.95399586 0.62923395 0.43122153\n",
      " 0.67813665 0.60915114 0.57461698 0.95399586 0.62347826        nan\n",
      "        nan 0.78728778 0.95395445 0.6378882 ]\n",
      "  warnings.warn(\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.97126686 0.61422604 0.96623604 0.54166989        nan 0.61925685\n",
      "        nan 0.78158892 0.90445579 0.64367861 0.42169619 0.88577396\n",
      " 0.38793739        nan 0.90446095 0.41380315 0.96623604 0.78591063\n",
      " 0.96623604 0.97055001 0.85774477 0.97126686 0.85486192 0.97126686\n",
      " 0.61566231 0.61279235 0.70832882 0.78520409 0.71337511 0.97126686\n",
      " 0.72846239 0.63578299 0.90086383 0.97055001 0.96623604 0.96623604\n",
      " 0.61925685 0.89223331        nan 0.62860163 0.63506356 0.96623604\n",
      " 0.7586447  0.46910858 0.97126686 0.63578299        nan 0.63793868\n",
      "        nan 0.97126686 0.52658776 0.96623604 0.59052113 0.96623604\n",
      " 0.96623604 0.63362471        nan        nan 0.50863051 0.97055001\n",
      " 0.96623604 0.52155953 0.64224234 0.96623604 0.63003791 0.85557876\n",
      " 0.6314716  0.43606147 0.58118666        nan 0.62931848 0.87281916\n",
      " 0.74639901 0.74070034 0.52874088 0.64367603 0.6400918  0.40374926\n",
      " 0.84050437        nan 0.77153245 0.97055001        nan 0.63506356\n",
      "        nan 0.97126686 0.49569892 0.97126686 0.63290529 0.44755421\n",
      " 0.69828009 0.605611   0.58190351 0.97055001 0.62284882        nan\n",
      "        nan 0.79955906 0.96623604 0.64511488]\n",
      "  warnings.warn(\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Grid for Logistic Regression\n",
    "\n",
    "param_grid_logr = [{\n",
    "     'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "     'solver': ['saga'],\n",
    "     'max_iter': np.arange(100,900),},\n",
    "      {\n",
    "     'penalty': ['l1', 'l2'],\n",
    "     'solver': ['liblinear'],\n",
    "     'max_iter': np.arange(100,900),},\n",
    "    {\n",
    "     'penalty': ['l2', 'none'],\n",
    "     'solver': ['lbfgs'],\n",
    "     'max_iter': np.arange(100,900),}\n",
    "]    \n",
    "\n",
    "rand_search_logr = RandomizedSearchCV(estimator = logreg, param_distributions=param_grid_logr, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Logistic Regression model fit for grid search\n",
    "_ = rand_search_logr.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_logr.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_logr.best_params_}\")\n",
    "\n",
    "bestRecallLogr = rand_search_logr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "The best recall score is 0.9453416149068324\n",
      "... with parameters: {'min_samples_split': 31, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.0021, 'max_leaf_nodes': 43, 'max_depth': 15, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.80198758 0.62335404 0.93097308 0.8852588  0.71536232 0.61192547\n",
      " 0.62335404 0.90815735 0.72401656 0.84186335 0.80488613 0.79039337\n",
      " 0.88530021 0.92815735 0.92530021 0.92815735 0.92244306 0.86484472\n",
      " 0.85341615 0.91101449 0.84766046 0.7152381  0.74443064 0.6578882\n",
      " 0.71536232 0.90530021 0.85341615 0.73809524 0.6405383  0.86815735\n",
      " 0.80488613 0.80488613 0.72401656 0.91101449 0.92815735 0.79888199\n",
      " 0.79039337 0.60592133 0.8389234  0.60592133 0.85668737 0.67755694\n",
      " 0.90815735 0.8447205         nan 0.79018634 0.79039337 0.92530021\n",
      " 0.94534161 0.69258799 0.65503106 0.90815735 0.71536232 0.66091097\n",
      " 0.77875776 0.73809524 0.67755694 0.77875776 0.71250518 0.88530021\n",
      " 0.87929607 0.73809524 0.79304348 0.65763975 0.73238095 0.81031056\n",
      " 0.91101449 0.90811594 0.61478261 0.91101449 0.92244306 0.73238095\n",
      " 0.84186335 0.80488613 0.91101449 0.71250518 0.85378882 0.78186335\n",
      " 0.87929607 0.81888199 0.80488613 0.6405383  0.6405383  0.8131677\n",
      " 0.91097308 0.80198758 0.60592133 0.81031056 0.71536232 0.62612836\n",
      " 0.90530021 0.85341615 0.7184265  0.72401656 0.82463768 0.71817805\n",
      " 0.89942029 0.79039337 0.61478261 0.79039337]\n",
      "  warnings.warn(\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.81172739 0.64867074 0.94612052 0.88717155 0.71410742 0.62131714\n",
      " 0.64867074 0.92383899 0.72911219 0.85631882 0.81316108 0.78950517\n",
      " 0.88786777 0.93391094 0.94038576 0.93391094 0.94395194 0.87643434\n",
      " 0.8599211  0.92455842 0.88071736 0.76287099 0.76649906 0.66886878\n",
      " 0.71554369 0.91160878 0.86279879 0.75652252 0.65952399 0.88499007\n",
      " 0.81316108 0.81316108 0.72911219 0.92455842 0.93391094 0.81243392\n",
      " 0.78590805 0.6271731  0.85419922 0.62645368 0.86127227 0.71054124\n",
      " 0.92383899 0.86637787        nan 0.81098734 0.78950517 0.94038576\n",
      " 0.95833011 0.6938604  0.66167453 0.92383899 0.71410742 0.69028132\n",
      " 0.78447693 0.75652252 0.71054124 0.78447693 0.70547175 0.88786777\n",
      " 0.88505969 0.75652252 0.8117145  0.6709729  0.76083907 0.81680462\n",
      " 0.92455842 0.91953018 0.61843944 0.92455842 0.93463036 0.76083907\n",
      " 0.85631882 0.81316108 0.92455842 0.7097883  0.86198912 0.80232072\n",
      " 0.88505969 0.82471829 0.81316108 0.65952399 0.65952399 0.81895516\n",
      " 0.9288827  0.81172739 0.62645368 0.81680462 0.71410742 0.64579304\n",
      " 0.91160878 0.8599211  0.72839277 0.72911219 0.84196127 0.75713365\n",
      " 0.91164488 0.78950517 0.61843944 0.78590805]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Grid for decision tree\n",
    "param_grid_tree = {\n",
    "    'min_samples_split': np.arange(1,100),  \n",
    "    'min_samples_leaf': np.arange(1,100),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 50), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "rand_search_tree = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid_tree, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Decision tree model fit for grid search\n",
    "_ = rand_search_tree.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_tree.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_tree.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 15 is smaller than n_iter=50. Running 15 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.9712629399585921\n",
      "... with parameters: {'kernel': 'linear', 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "# Grid for SVM\n",
    "param_grid_svm = [{\n",
    "    'degree': [2,3],\n",
    "    'C': [1,5,10],\n",
    "    'kernel': ['poly'],   \n",
    "},\n",
    "{\n",
    "    'C': [1,5,10],\n",
    "    'gamma': [1, 0.1],\n",
    "    'kernel': ['rbf'],   \n",
    "},\n",
    "{\n",
    "    'C': [1,5,10],\n",
    "    'kernel': ['linear'],  \n",
    "}]\n",
    "\n",
    "rand_search_svm = RandomizedSearchCV(estimator = svmm, param_distributions=param_grid_svm, cv=kfolds, n_iter=50,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# SVM model fit for grid search\n",
    "_ = rand_search_svm.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_svm.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_svm.best_params_}\")\n",
    "\n",
    "bestRecallSvm = rand_search_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 20 is smaller than n_iter=500. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.959792960662526\n",
      "... with parameters: {'n_estimators': 1000, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#Grid for ADABoost Classifier\n",
    "\n",
    "param_grid_ada = {  \n",
    "     'n_estimators': [10,50,250,1000,2000],\n",
    "     'learning_rate': [0.01,0.1,0.2,1.0],}   \n",
    "\n",
    "rand_search_ada = RandomizedSearchCV(estimator = adatree, param_distributions=param_grid_ada, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# ADABoost Classifier model fit for grid search\n",
    "_ = rand_search_ada.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_ada.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_ada.best_params_}\")\n",
    "\n",
    "bestRecallAda = rand_search_ada.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 180 is smaller than n_iter=500. Running 180 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "300 fits failed out of a total of 900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "300 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.82778468 0.80749482 0.82757764 0.80165631 0.79879917 0.83619048\n",
      " 0.82749482 0.81602484 0.79879917 0.79879917 0.77867495 0.81325052\n",
      " 0.79879917 0.79594203 0.79594203 0.92248447 0.91097308 0.92521739\n",
      " 0.92815735 0.92811594 0.86496894 0.90517598 0.92252588 0.91664596\n",
      " 0.93391304 0.89362319 0.90227743 0.92231884 0.90231884 0.90517598\n",
      " 0.94538302 0.93672878 0.94538302 0.94538302 0.94538302 0.93113872\n",
      " 0.93672878 0.94538302 0.94538302 0.94538302 0.90803313 0.94538302\n",
      " 0.94538302 0.94538302 0.94538302 0.9310559  0.94824017 0.95113872\n",
      " 0.95399586 0.95113872 0.93672878 0.94534161 0.95399586 0.95113872\n",
      " 0.95399586 0.93962733 0.95399586 0.95113872 0.95399586 0.95399586\n",
      " 0.8594617  0.81904762 0.82182195 0.80741201 0.80741201 0.79014493\n",
      " 0.82753623 0.81594203 0.82455487 0.80455487 0.8131677  0.79308489\n",
      " 0.79308489 0.80451346 0.80165631 0.88509317 0.913706   0.9194617\n",
      " 0.92811594 0.92521739 0.89362319 0.91374741 0.92240166 0.92530021\n",
      " 0.92521739 0.87362319 0.89652174 0.89937888 0.90517598 0.89942029\n",
      " 0.93971014 0.95113872 0.95109731 0.95399586 0.94828157 0.936853\n",
      " 0.94542443 0.94824017 0.94824017 0.94824017 0.92815735 0.93962733\n",
      " 0.94824017 0.94248447 0.94248447 0.94252588 0.95689441 0.95689441\n",
      " 0.95979296 0.95689441 0.92828157 0.94824017 0.95689441 0.95689441\n",
      " 0.95399586 0.92521739 0.94824017 0.95689441 0.95399586 0.95399586\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.85415281 0.82902452 0.82972332 0.82828705 0.82973621 0.83407081\n",
      " 0.84410149 0.82686883 0.83117248 0.83476445 0.82113406 0.83834868\n",
      " 0.81538124 0.81537093 0.81680204 0.93389289 0.93820943 0.94611794\n",
      " 0.9446791  0.94396225 0.92095614 0.93391867 0.94539852 0.94396225\n",
      " 0.94396225 0.90375442 0.93677574 0.94396225 0.93893917 0.9425234\n",
      " 0.96695805 0.96264408 0.96551404 0.96192465 0.96264408 0.96118976\n",
      " 0.96982801 0.96480235 0.9633635  0.96264408 0.95546015 0.96336092\n",
      " 0.96767231 0.96264408 0.96192465 0.98204791 0.98634641 0.9906578\n",
      " 0.99066037 0.98993837 0.98204275 0.9885021  0.98706841 0.99066037\n",
      " 0.98922153 0.98205049 0.98634899 0.9906578  0.98922153 0.9892241\n",
      " 0.8735128  0.83905521 0.83908357 0.83405018 0.83620072 0.81175834\n",
      " 0.82687141 0.83692272 0.84483123 0.83331786 0.81609551 0.82183028\n",
      " 0.81679689 0.82039659 0.81966943 0.92094582 0.94539852 0.94468167\n",
      " 0.94396225 0.94396225 0.93176298 0.94253114 0.94396225 0.94468167\n",
      " 0.94396225 0.90443    0.92744385 0.93821459 0.94037286 0.93965086\n",
      " 0.97630283 0.97485882 0.97557567 0.97629509 0.97629509 0.97916248\n",
      " 0.97413424 0.97557309 0.97557567 0.97629509 0.96838658 0.97342255\n",
      " 0.97557567 0.97413682 0.97557567 0.9856373  0.99424976 0.99569119\n",
      " 0.99641061 0.99497434 0.98707357 0.99496919 0.99569119 0.99353549\n",
      " 0.99497434 0.98923184 0.99281865 0.99425234 0.99425492 0.99425492\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:926: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.959792960662526\n",
      "... with parameters: {'n_estimators': 1000, 'max_features': 'auto', 'max_depth': 10, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "#Grid for Randomforest Classifier\n",
    "\n",
    "param_grid_rf = {  \n",
    "     'n_estimators': [10,50,250,1000,2000],\n",
    "     'max_features': ['auto', 'sqrt', 'log2'],\n",
    "     'max_depth' : [4,6,8,10],\n",
    "     'criterion' :['gini', 'entropy', 'log_loss'],}   \n",
    "\n",
    "rand_search_rf = RandomizedSearchCV(estimator = rforest, param_distributions=param_grid_rf, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# ADABoost Classifier model fit for grid search\n",
    "_ = rand_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_rf.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_rf.best_params_}\")\n",
    "\n",
    "bestRecallRf = rand_search_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 200 is smaller than n_iter=500. Running 200 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "The best recall score is 0.9655072463768116\n",
      "... with parameters: {'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.2}\n"
     ]
    }
   ],
   "source": [
    "#Grid for XGBoost Classifier\n",
    "\n",
    "param_grid_xg = {  \n",
    "    'max_depth': range (2, 10, 1),\n",
    "    'n_estimators': [10,50,250,1000,2000],\n",
    "    'learning_rate': [1.0,0.2,0.1, 0.01, 0.05],}   \n",
    "\n",
    "rand_search_xg = RandomizedSearchCV(estimator = xgboost, param_distributions=param_grid_xg, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# XGBoost Classifier model fit for grid search\n",
    "_ = rand_search_xg.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_xg.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_xg.best_params_}\")\n",
    "\n",
    "bestRecallXg = rand_search_xg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best recall score is 0.9655072463768116\n",
      "... with parameters: {'n_estimators': 10, 'min_samples_split': 8, 'min_samples_leaf': 10, 'min_impurity_decrease': 0.0096, 'max_depth': 9, 'loss': 'exponential', 'learning_rate': 1.0, 'criterion': 'friedman_mse'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "980 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "905 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 525, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 282, in _check_params\n",
      "    raise ValueError(\"Loss '{0:s}' not supported. \".format(self.loss))\n",
      "ValueError: Loss 'log_loss' not supported. \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 586, in fit\n",
      "    n_stages = self._fit_stages(\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 663, in _fit_stages\n",
      "    raw_predictions = self._fit_stage(\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 246, in _fit_stage\n",
      "    tree.fit(X, residual, sample_weight=sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1315, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.93097308 0.9194617  0.         0.93966874        nan\n",
      " 0.90517598 0.956853   0.         0.95693582        nan        nan\n",
      " 0.91954451 0.94538302        nan        nan 0.93097308 0.95693582\n",
      " 0.91954451        nan 0.94819876        nan        nan        nan\n",
      "        nan 0.9626501  0.9626501         nan        nan 0.9626501\n",
      " 0.92815735 0.80111801        nan        nan 0.93966874 0.94248447\n",
      "        nan 0.89656315        nan 0.94828157 0.90811594        nan\n",
      "        nan        nan 0.9252588  0.91954451        nan        nan\n",
      "        nan 0.92819876        nan 0.91101449        nan 0.95407867\n",
      " 0.         0.91101449        nan 0.93677019 0.95693582 0.92815735\n",
      " 0.9626501  0.95689441 0.95693582        nan 0.91093168        nan\n",
      " 0.83598344        nan 0.95399586 0.95399586        nan        nan\n",
      "        nan 0.93391304 0.95975155 0.95395445 0.92815735 0.91101449\n",
      " 0.95399586 0.90811594 0.95399586 0.95693582 0.90811594 0.\n",
      "        nan        nan        nan 0.95975155 0.93391304        nan\n",
      "        nan 0.92815735        nan 0.9252588         nan 0.91101449\n",
      "        nan 0.9626501  0.79581781        nan 0.93962733        nan\n",
      "        nan 0.96550725        nan        nan 0.86207039 0.91097308\n",
      "        nan 0.95399586 0.95689441 0.82703934 0.93391304 0.81287785\n",
      " 0.9252588         nan 0.9252588         nan 0.95399586 0.94252588\n",
      " 0.95407867        nan        nan        nan 0.9626501  0.93971014\n",
      "        nan 0.93966874        nan 0.94542443 0.95113872 0.88799172\n",
      " 0.95395445 0.         0.93962733 0.93101449 0.95689441 0.94542443\n",
      "        nan 0.9252588  0.72124224        nan        nan 0.82447205\n",
      " 0.95975155 0.94542443 0.91101449 0.95693582 0.95689441 0.86778468\n",
      "        nan        nan        nan        nan 0.85908903 0.93395445\n",
      "        nan 0.90231884        nan 0.93677019        nan 0.95689441\n",
      " 0.95693582        nan 0.95399586 0.92815735 0.95689441        nan\n",
      " 0.95689441        nan 0.95689441 0.95979296 0.93101449        nan\n",
      " 0.95689441 0.92240166 0.95979296 0.9252588         nan        nan\n",
      "        nan 0.95693582        nan 0.93677019 0.9252588  0.95403727\n",
      " 0.95689441 0.7494824  0.95113872 0.95693582 0.94538302        nan\n",
      "        nan 0.                nan        nan 0.93395445 0.95403727\n",
      " 0.95979296        nan 0.95403727        nan 0.95693582 0.89080745\n",
      "        nan 0.95693582        nan 0.95113872        nan 0.9310559\n",
      " 0.92811594 0.         0.93101449 0.92815735 0.         0.88513458\n",
      "        nan 0.91378882 0.95689441        nan        nan 0.95109731\n",
      " 0.93391304 0.                nan        nan 0.92236025 0.95403727\n",
      " 0.95403727 0.92538302 0.95689441 0.95975155 0.76712215 0.90811594\n",
      "        nan 0.95689441 0.                nan 0.95399586 0.86492754\n",
      "        nan 0.91101449        nan        nan 0.95399586 0.\n",
      " 0.956853   0.         0.91664596 0.95399586 0.93966874        nan\n",
      " 0.94832298 0.95979296        nan 0.93395445        nan 0.95693582\n",
      " 0.72124224        nan        nan        nan 0.91664596        nan\n",
      "        nan 0.         0.956853   0.93395445 0.92521739        nan\n",
      " 0.89652174        nan 0.95975155        nan        nan 0.90811594\n",
      " 0.                nan        nan        nan 0.94824017 0.9252588\n",
      "        nan 0.95979296 0.92815735 0.90811594 0.         0.9252588\n",
      " 0.9510559  0.92240166 0.91101449 0.95971014        nan        nan\n",
      "        nan 0.95689441 0.90795031        nan 0.94256729 0.95693582\n",
      "        nan        nan        nan 0.81594203        nan        nan\n",
      " 0.93391304        nan        nan        nan        nan 0.\n",
      "        nan 0.93681159 0.91950311        nan 0.                nan\n",
      " 0.96550725 0.9626501         nan 0.91101449        nan        nan\n",
      "        nan 0.92236025 0.94538302        nan        nan 0.93958592\n",
      " 0.95403727        nan 0.94256729 0.95689441        nan 0.93391304\n",
      " 0.90811594        nan 0.9252588  0.94538302 0.         0.92240166\n",
      "        nan 0.94248447 0.93391304 0.95975155        nan 0.91093168\n",
      " 0.95403727 0.90231884 0.9310559         nan        nan 0.9626501\n",
      " 0.9626087  0.93966874        nan 0.95403727        nan        nan\n",
      " 0.92811594 0.9626501  0.90803313        nan 0.9310559  0.82169772\n",
      " 0.                nan        nan        nan 0.95693582 0.91101449\n",
      "        nan 0.94252588        nan        nan 0.92815735 0.90803313\n",
      " 0.         0.95693582        nan 0.95407867 0.95399586 0.85929607\n",
      "        nan 0.93681159        nan 0.         0.85929607 0.9626087\n",
      " 0.95979296 0.91093168 0.91101449        nan 0.94819876        nan\n",
      " 0.9252588  0.95113872        nan 0.91954451 0.91101449 0.\n",
      " 0.9252588  0.9626501         nan 0.83623188 0.95689441 0.95979296\n",
      " 0.93971014 0.95407867        nan        nan 0.95693582        nan\n",
      "        nan        nan 0.94538302        nan        nan 0.92240166\n",
      " 0.95979296 0.                nan 0.94828157 0.90811594        nan\n",
      "        nan        nan 0.95979296        nan        nan 0.93101449\n",
      " 0.         0.7926294         nan        nan        nan 0.9252588\n",
      " 0.95693582 0.95113872        nan 0.93097308        nan 0.\n",
      "        nan        nan        nan 0.94542443 0.88207039 0.95979296\n",
      "        nan 0.91950311        nan        nan        nan 0.91664596\n",
      " 0.95113872        nan 0.                nan        nan        nan\n",
      "        nan        nan 0.83623188 0.93966874 0.95693582        nan\n",
      " 0.         0.95689441 0.93093168 0.95979296        nan 0.93391304\n",
      " 0.93097308        nan 0.         0.8821118         nan        nan\n",
      "        nan 0.91383023 0.95693582 0.95979296        nan        nan\n",
      " 0.9252588         nan 0.90811594 0.95681159 0.94538302 0.90811594\n",
      " 0.94248447        nan 0.93962733 0.93681159 0.9252588         nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan 0.98133364 0.92169877 0.         0.95402388        nan\n",
      " 0.9087646  0.9820376  0.         0.99712746        nan        nan\n",
      " 0.92816328 0.97628994        nan        nan 0.95044223 0.99784688\n",
      " 0.92959697        nan 0.99856373        nan        nan        nan\n",
      "        nan 0.99712746 0.99712746        nan        nan 0.99856373\n",
      " 0.93821975 0.81979578        nan        nan 0.95544983 0.99281607\n",
      "        nan 0.90661149        nan 0.96407777 0.92096903        nan\n",
      "        nan        nan 0.92959697 0.92959697        nan        nan\n",
      "        nan 0.9425234         nan 0.92455842        nan 0.9798819\n",
      " 0.         0.92455842        nan 0.99496919 0.99712746 0.93821975\n",
      " 0.99712746 0.99856373 0.99856373        nan 0.92168846        nan\n",
      " 0.83985715        nan 0.99856373 0.99856373        nan        nan\n",
      "        nan 0.95042676 0.99712746 0.9798819  0.9396457  0.92455842\n",
      " 0.99209922 0.92096903 0.98563988 0.99712746 0.92096903 0.\n",
      "        nan        nan        nan 0.99712746 0.95042676        nan\n",
      "        nan 0.94396483        nan 0.92816328        nan 0.92455842\n",
      "        nan 0.99712746 0.81250097        nan 0.9518656         nan\n",
      "        nan 0.99856373        nan        nan 0.86995694 0.92599985\n",
      "        nan 0.97054228 0.99928058 0.8434981  0.95258245 0.82981099\n",
      " 0.92959697        nan 0.92959697        nan 0.99712746 0.95401614\n",
      " 0.99856373        nan        nan        nan 0.99712746 0.95760553\n",
      "        nan 0.9590418         nan 0.96767231 0.99712746 0.90374152\n",
      " 0.99568861 0.         0.96049612 0.93964828 0.99712746 0.98563472\n",
      "        nan 0.92959697 0.73848018        nan        nan 0.83985199\n",
      " 0.99712746 0.97341482 0.92455842 0.99784431 0.98204275 0.87787319\n",
      "        nan        nan        nan        nan 0.88651144 0.94180398\n",
      "        nan 0.91162683        nan 0.95545241        nan 0.99712746\n",
      " 0.99712746        nan 0.99856373 0.93246435 0.99569377        nan\n",
      " 0.99712746        nan 0.99784688 0.99712746 0.93605632        nan\n",
      " 0.99641061 0.93893659 0.99712746 0.92959697        nan        nan\n",
      "        nan 0.99856373        nan 0.94682963 0.92672443 0.99712746\n",
      " 0.99712746 0.76807199 0.99281865 0.99712746 0.95474072        nan\n",
      "        nan 0.                nan        nan 0.97629509 0.98346871\n",
      " 0.99784431        nan 0.99712746        nan 0.99712746 0.91307083\n",
      "        nan 0.99712746        nan 0.99712746        nan 0.94323767\n",
      " 0.93677574 0.         0.93964312 0.93031381 0.         0.90660633\n",
      "        nan 0.92169103 0.99712746        nan        nan 0.99712746\n",
      " 0.95617699 0.                nan        nan 0.93247467 0.99856373\n",
      " 0.98275702 0.94252082 0.99209922 0.99928058 0.78952838 0.91162425\n",
      "        nan 0.99928315 0.                nan 0.99712746 0.88146257\n",
      "        nan 0.92455842        nan        nan 0.98419587 0.\n",
      " 0.99712746 0.         0.92241819 0.99712746 0.95976122        nan\n",
      " 0.99713004 0.99712746        nan 0.95114618        nan 0.98850726\n",
      " 0.73848018        nan        nan        nan 0.92743612        nan\n",
      "        nan 0.         0.99640803 0.9425234  0.94325056        nan\n",
      " 0.90015214        nan 0.99784688        nan        nan 0.92096903\n",
      " 0.                nan        nan        nan 0.95761326 0.92959697\n",
      "        nan 0.99856373 0.93391094 0.92240272 0.         0.92959697\n",
      " 0.99712746 0.92456873 0.92455842 0.99712746        nan        nan\n",
      "        nan 0.99712746 0.91882881        nan 0.96193497 0.99712746\n",
      "        nan        nan        nan 0.83983394        nan        nan\n",
      " 0.95330187        nan        nan        nan        nan 0.\n",
      "        nan 0.95113844 0.92960212        nan 0.                nan\n",
      " 0.99712746 0.99856373        nan 0.92455842        nan        nan\n",
      "        nan 0.9281607  0.99712488        nan        nan 0.99712746\n",
      " 0.99712746        nan 0.96551404 0.99712746        nan 0.94324798\n",
      " 0.92096903        nan 0.92959697 0.96192208 0.         0.92456615\n",
      "        nan 0.98346871 0.94970475 0.99856373        nan 0.92384931\n",
      " 0.99712746 0.90301952 0.9439803         nan        nan 0.99712746\n",
      " 0.99712746 0.95689642        nan 0.99712746        nan        nan\n",
      " 0.95259534 0.99712746 0.92026508        nan 0.94036513 0.83625745\n",
      " 0.                nan        nan        nan 0.9820376  0.92455842\n",
      "        nan 0.95761069        nan        nan 0.93031381 0.92241046\n",
      " 0.         0.99712746        nan 1.         0.97341997 0.89007246\n",
      "        nan 0.94970217        nan 0.         0.87069699 0.99568861\n",
      " 0.99712746 0.92384931 0.92455842        nan 0.97197339        nan\n",
      " 0.9288827  0.96049354        nan 0.9317604  0.93391094 0.\n",
      " 0.92959697 0.99712746        nan 0.8649287  0.99712746 0.99712746\n",
      " 0.97271344 0.99712746        nan        nan 0.99712746        nan\n",
      "        nan        nan 0.98131559        nan        nan 0.92959697\n",
      " 0.99712746 0.                nan 0.97270571 0.92096903        nan\n",
      "        nan        nan 0.99712746        nan        nan 0.93964828\n",
      " 0.         0.80755525        nan        nan        nan 0.92959697\n",
      " 0.99712746 0.99784688        nan 0.93175524        nan 0.\n",
      "        nan        nan        nan 0.95401614 0.89083572 0.99784688\n",
      "        nan 0.92241304        nan        nan        nan 0.93319409\n",
      " 0.99066295        nan 0.                nan        nan        nan\n",
      "        nan        nan 0.8649287  0.95401614 0.99712746        nan\n",
      " 0.         0.99784431 0.95760295 0.99712746        nan 0.94395967\n",
      " 0.93318893        nan 0.         0.89726155        nan        nan\n",
      "        nan 0.93104871 0.99712746 0.99712746        nan        nan\n",
      " 0.93965344        nan 0.9109074  0.99712746 0.96263892 0.92240788\n",
      " 0.95906243        nan 0.9619195  0.94899049 0.92672443        nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Grid for Gradient Boost Classifier\n",
    "\n",
    "param_grid_gb = {  \n",
    "    'min_samples_split': np.arange(1,20),  \n",
    "    'min_samples_leaf': np.arange(1,12),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005), \n",
    "    'loss': ['log_loss', 'deviance', 'exponential'],\n",
    "    'criterion': ['friedman_mse', 'squared_error'],\n",
    "    'max_depth': range (2, 10, 1),\n",
    "    'n_estimators': [10,50,250,1000,2000],\n",
    "    'learning_rate': [1.0,0.2,0.1, 0.01, 0.05],}   \n",
    "\n",
    "rand_search_gb = RandomizedSearchCV(estimator = gboost, param_distributions=param_grid_gb, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Gradient Boost Classifier model fit for grid search\n",
    "_ = rand_search_gb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_gb.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_gb.best_params_}\")\n",
    "\n",
    "bestRecallgb = rand_search_gb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================LOGISTIC REGRESSION====================\n",
      "\n",
      "The best recall score is 0.9539958592132505\n",
      "... with parameters: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 306}\n",
      "\n",
      "=========================DECISION TREE==========================\n",
      "\n",
      "The best recall score is 0.9453416149068324\n",
      "... with parameters: {'min_samples_split': 31, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.0021, 'max_leaf_nodes': 43, 'max_depth': 15, 'criterion': 'entropy'}\n",
      "\n",
      "==============================SVM===============================\n",
      "\n",
      "The best recall score is 0.9712629399585921\n",
      "... with parameters: {'kernel': 'linear', 'C': 1}\n",
      "\n",
      "=========================ADABOOST===============================\n",
      "\n",
      "The best recall score is 0.959792960662526\n",
      "... with parameters: {'n_estimators': 1000, 'learning_rate': 0.1}\n",
      "\n",
      "=========================RANDOMFOREST===========================\n",
      "\n",
      "The best recall score is 0.959792960662526\n",
      "... with parameters: {'n_estimators': 1000, 'max_features': 'auto', 'max_depth': 10, 'criterion': 'entropy'}\n",
      "\n",
      "=========================XGBOOST================================\n",
      "\n",
      "The best recall score is 0.9655072463768116\n",
      "... with parameters: {'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.2}\n",
      "\n",
      "=========================GRADIENT BOOST================================\n",
      "\n",
      "The best recall score is 0.9655072463768116\n",
      "... with parameters: {'n_estimators': 10, 'min_samples_split': 8, 'min_samples_leaf': 10, 'min_impurity_decrease': 0.0096, 'max_depth': 9, 'loss': 'exponential', 'learning_rate': 1.0, 'criterion': 'friedman_mse'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=========================LOGISTIC REGRESSION====================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_logr.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_logr.best_params_}\")\n",
    "print(\"\\n=========================DECISION TREE==========================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_tree.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_tree.best_params_}\")\n",
    "print(\"\\n==============================SVM===============================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_svm.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_svm.best_params_}\")\n",
    "print(\"\\n=========================ADABOOST===============================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_ada.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_ada.best_params_}\")\n",
    "print(\"\\n=========================RANDOMFOREST===========================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_rf.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_rf.best_params_}\")\n",
    "print(\"\\n=========================XGBOOST================================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_xg.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_xg.best_params_}\")\n",
    "print(\"\\n=========================GRADIENT BOOST================================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_gb.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_gb.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Grid Search ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.959792960662526\n",
      "... with parameters: {'learning_rate': 0.07, 'n_estimators': 800}\n"
     ]
    }
   ],
   "source": [
    "# ADABoosting classifier grid\n",
    "param_grid_ada = {  \n",
    "     'n_estimators': [800,1000,1200],\n",
    "     'learning_rate': [0.07,0.1,0.13],}  \n",
    "\n",
    "\n",
    "grid_search_ada = GridSearchCV(estimator = adatree, param_grid=param_grid_ada, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# ADABoosting classifier fit\n",
    "_ = grid_search_ada.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_ada.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_ada.best_params_}\")\n",
    "\n",
    "bestRecallAda = grid_search_ada.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "The best recall score is 0.9655072463768116\n",
      "... with parameters: {'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "#Grid for XGBoost Classifier\n",
    "\n",
    "param_grid_xg = {  \n",
    "    'max_depth': [2,4,6],\n",
    "    'n_estimators': [200,250,300],\n",
    "    'learning_rate': [0.17,0.2,0.23],}   \n",
    "\n",
    "grid_search_xg = GridSearchCV(estimator = xgboost, param_grid=param_grid_xg, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# XGBoost Classifier model fit for grid search\n",
    "_ = grid_search_xg.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_xg.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_xg.best_params_}\")\n",
    "\n",
    "bestRecallXg = grid_search_xg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:926: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.9568944099378882\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 60}\n"
     ]
    }
   ],
   "source": [
    "#Grid for Randomforest Classifier\n",
    "param_grid_rf = {  \n",
    "     'n_estimators': [40,50,60],\n",
    "     'max_features': ['sqrt'],\n",
    "     'max_depth' : [8,10,12],\n",
    "     'criterion' :['entropy'],}   \n",
    "\n",
    "grid_search_rf = GridSearchCV(estimator = rforest, param_grid=param_grid_rf, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# ADABoost Classifier model fit for grid search\n",
    "_ = grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_rf.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_rf.best_params_}\")\n",
    "\n",
    "bestRecallRf = grid_search_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.9712629399585921\n",
      "... with parameters: {'C': 1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Grid for SVM\n",
    "param_grid_svm = {\n",
    "    'C': [1,2,3],\n",
    "    'kernel': ['linear'],  \n",
    "}\n",
    "\n",
    "grid_search_svm = GridSearchCV(estimator = svmm, param_grid=param_grid_svm, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# SVM model fit for grid search\n",
    "_ = grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_svm.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_svm.best_params_}\")\n",
    "\n",
    "bestRecallSvm = grid_search_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "The best recall score is 0.94824016563147\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 13, 'max_leaf_nodes': 41, 'min_impurity_decrease': 0.0018, 'min_samples_leaf': 2, 'min_samples_split': 29}\n"
     ]
    }
   ],
   "source": [
    "# Grid for decision tree\n",
    " \n",
    "param_grid_tree = {\n",
    "    'min_samples_split': [29,31,33],  \n",
    "    'min_samples_leaf': [2,4,6],\n",
    "    'min_impurity_decrease': [0.0018,0.0021,0.0024],\n",
    "    'max_leaf_nodes': [41,43,45], \n",
    "    'max_depth': [13,15,17], \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "grid_search_tree = GridSearchCV(estimator = dtree, param_grid=param_grid_tree, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Decision tree model fit for grid search\n",
    "_ = grid_search_tree.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_tree.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_tree.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "The best recall score is 0.9539958592132505\n",
      "... with parameters: {'max_iter': 180, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Grid for Logistic Regression\n",
    "\n",
    "param_grid_logr = {\n",
    "     'penalty': ['l1'],\n",
    "     'solver': ['liblinear'],\n",
    "     'max_iter': [180,200,220],\n",
    "}\n",
    "\n",
    "grid_search_logr = GridSearchCV(estimator = logreg, param_grid=param_grid_logr, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Logistic Regression model fit for grid search\n",
    "_ = grid_search_logr.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_logr.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_logr.best_params_}\")\n",
    "\n",
    "bestRecallLogr = grid_search_logr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.9683643892339544\n",
      "... with parameters: {'criterion': 'friedman_mse', 'learning_rate': 0.2, 'loss': 'deviance', 'max_depth': 5, 'min_impurity_decrease': 0.0038, 'min_samples_leaf': 7, 'min_samples_split': 13, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "#Grid for Gradient Boost Classifier\n",
    "\n",
    "param_grid_gb = {  \n",
    "    'min_samples_split': [13,15,17],  \n",
    "    'min_samples_leaf': [5,7,9],\n",
    "    'min_impurity_decrease': [0.0038,0.0041,0.0044], \n",
    "    'loss': ['deviance'],\n",
    "    'criterion': ['friedman_mse'],\n",
    "    'max_depth': [3,5,7],\n",
    "    'n_estimators': [200,250,300],\n",
    "    'learning_rate': [0.17,0.2,0.23],}   \n",
    "\n",
    "grid_search_gb = GridSearchCV(estimator = gboost, param_grid=param_grid_gb, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Gradient Boost Classifier model fit for grid search\n",
    "_ = grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_gb.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_gb.best_params_}\")\n",
    "\n",
    "bestRecallgb = rand_search_gb.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final models with best parameters ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(criterion='entropy', max_depth=13, max_leaf_nodes=41, min_impurity_decrease=0.0018, min_samples_leaf=2, min_samples_split=29)\n",
    "svmm = SVC(C=1, kernel='linear')\n",
    "logreg = LogisticRegression(max_iter=180, penalty='l1', solver='liblinear')\n",
    "adatree = AdaBoostClassifier(learning_rate=0.07, n_estimators=800)\n",
    "rforest = RandomForestClassifier(criterion='entropy', max_depth=10, max_features='sqrt', n_estimators=50)\n",
    "xgboost = XGBClassifier(learning_rate=0.2, max_depth=4, n_estimators=250)\n",
    "gboost = GradientBoostingClassifier(criterion='friedman_mse', learning_rate=0.17, loss='deviance', max_depth=7, min_impurity_decrease=0.0038, min_samples_leaf=5, min_samples_split=17, n_estimators=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fit for train dataset and prediction with test dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.959349593495935\n",
      "Accuracy Score:   0.9887218045112782\n",
      "Precision Score:  0.9915966386554622\n",
      "F1 Score:         0.9752066115702479\n"
     ]
    }
   ],
   "source": [
    "_ = xgboost.fit(X_train, y_train)\n",
    "y_pred = xgboost.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "xgboost_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\AppData\\Local\\Temp\\ipykernel_25488\\214086440.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  _ = rforest.fit(X_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.967479674796748\n",
      "Accuracy Score:   0.9906015037593985\n",
      "Precision Score:  0.9916666666666667\n",
      "F1 Score:         0.9794238683127573\n"
     ]
    }
   ],
   "source": [
    "_ = rforest.fit(X_train, y_train)\n",
    "y_pred = rforest.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "rforest_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.983739837398374\n",
      "Accuracy Score:   0.9962406015037594\n",
      "Precision Score:  1.0\n",
      "F1 Score:         0.9918032786885246\n"
     ]
    }
   ],
   "source": [
    "_ = adatree.fit(X_train, y_train)\n",
    "y_pred = adatree.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "adatree_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.975609756097561\n",
      "Accuracy Score:   0.9906015037593985\n",
      "Precision Score:  0.9836065573770492\n",
      "F1 Score:         0.9795918367346939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "_ = logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "logreg_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.975609756097561\n",
      "Accuracy Score:   0.9943609022556391\n",
      "Precision Score:  1.0\n",
      "F1 Score:         0.9876543209876543\n"
     ]
    }
   ],
   "source": [
    "_ = svmm.fit(X_train, y_train)\n",
    "y_pred = svmm.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "svmm_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.975609756097561\n",
      "Accuracy Score:   0.9868421052631579\n",
      "Precision Score:  0.967741935483871\n",
      "F1 Score:         0.97165991902834\n"
     ]
    }
   ],
   "source": [
    "_ = dtree.fit(X_train, y_train)\n",
    "y_pred = dtree.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "dtree_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.975609756097561\n",
      "Accuracy Score:   0.9943609022556391\n",
      "Precision Score:  1.0\n",
      "F1 Score:         0.9876543209876543\n"
     ]
    }
   ],
   "source": [
    "_ = gboost.fit(X_train, y_train)\n",
    "y_pred = gboost.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "gboost_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall scores...\n",
      "Decision Tree:           0.975609756097561\n",
      "SVM:                     0.975609756097561\n",
      "Logistic Regression      0.975609756097561\n",
      "Random Forest:           0.967479674796748\n",
      "Ada Boosted Tree:        0.983739837398374\n",
      "XGBoost Tree:            0.959349593495935\n",
      "Gradient Boosting        0.991869918699187\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall scores...\")\n",
    "print(f\"{'Decision Tree:':25}{dtree_recall}\")\n",
    "print(f\"{'SVM:':25}{svmm_recall}\")\n",
    "print(f\"{'Logistic Regression':25}{logreg_recall}\")\n",
    "\n",
    "print(f\"{'Random Forest:':25}{rforest_recall}\")\n",
    "print(f\"{'Ada Boosted Tree:':25}{adatree_recall}\")\n",
    "print(f\"{'XGBoost Tree:':25}{xgboost_recall}\")\n",
    "print(f\"{'Gradient Boosting':25}{gboost_recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Models ##\n",
    "\n",
    "The analysis of recall score of models show that Grading Boosting is the best model (0.992). \n",
    "While XGBoost performs worst (0.959), the Decision Tree, SVM, and Logistic Regression perform same (0.976). The random forest performs better than XGBoost (.967), but not better than most of the models.\n",
    "\n",
    "This shows that the esemble models perform better than the pruning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model to disk ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save model\n",
    "pickle.dump(gboost, open('./gboost_model.pkl', \"wb\"))\n",
    "\n",
    "# If you wish to load this model later, simply use pickle.load method\n",
    "#loaded_model = pickle.load(open('logistic_model_example01.pkl', \"rb\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Class08b-decision_tree_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b056086e24cb5602cbcb82122035cd3d6ee2ccbf5df29c16e348c108b0f83be3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
