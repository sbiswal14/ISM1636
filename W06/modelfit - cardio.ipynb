{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxGJbLsUhuc8"
   },
   "source": [
    "## Subash Chandra Biswal (U77884251) ##\n",
    "# Assignment 1 - Cardiotocography\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tuXRZKEYrDa"
   },
   "source": [
    "## Introduction and Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q08EVUytY3eh"
   },
   "source": [
    "Author: J. P. Marques de Sá, J. Bernardes, D. Ayers de Campos.  \n",
    "Source: UCI  \n",
    "Please cite: Ayres de Campos et al. (2000) SisPorto 2.0 A Program for Automated Analysis of Cardiotocograms. J Matern Fetal Med 5:311-318, UCI    \n",
    "\n",
    "2126 fetal cardiotocograms (CTGs) were automatically processed and the respective diagnostic features measured. The CTGs were also classified by three expert obstetricians and a consensus classification label assigned to each of them. Classification was both with respect to a morphologic pattern (A, B, C. ...) and to a fetal state (N, S, P). Therefore the dataset can be used either for 10-class or 3-class experiments.  \n",
    "\n",
    "Attribute Information:  \n",
    "LB - FHR baseline (beats per minute)  \n",
    "AC - # of accelerations per second  \n",
    "FM - # of fetal movements per second  \n",
    "UC - # of uterine contractions per second  \n",
    "DL - # of light decelerations per second  \n",
    "DS - # of severe decelerations per second  \n",
    "DP - # of prolongued decelerations per second  \n",
    "ASTV - percentage of time with abnormal short term variability  \n",
    "MSTV - mean value of short term variability  \n",
    "ALTV - percentage of time with abnormal long term variability  \n",
    "MLTV - mean value of long term variability  \n",
    "Width - width of FHR histogram  \n",
    "Min - minimum of FHR histogram  \n",
    "Max - Maximum of FHR histogram  \n",
    "Nmax - # of histogram peaks  \n",
    "Nzeros - # of histogram zeros  \n",
    "Mode - histogram mode  \n",
    "Mean - histogram mean  \n",
    "Median - histogram median  \n",
    "Variance - histogram variance  \n",
    "Tendency - histogram tendency  \n",
    "CLASS - FHR pattern class code (1 to 10)  \n",
    "NSP - fetal state class code (N=normal(1); S=suspect(2); P=pathologic(3))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmYLcm3aY8X5"
   },
   "source": [
    "## Install and import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8zNdljvIhuc8"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "# set random seed to ensure that results are repeatable\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGgrXNQPZT3J"
   },
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "q3u5LsGyhudA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V28</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "      <th>V31</th>\n",
       "      <th>V32</th>\n",
       "      <th>V33</th>\n",
       "      <th>V34</th>\n",
       "      <th>V35</th>\n",
       "      <th>Class</th>\n",
       "      <th>NSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>240</td>\n",
       "      <td>357</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>632</td>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>177</td>\n",
       "      <td>779</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>411</td>\n",
       "      <td>1192</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>2.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>533</td>\n",
       "      <td>1147</td>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>2.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   V1   V2    V3   V4   V5  V6  V7  V8  V9  V10  ...  V28  V29  V30  V31  V32  \\\n",
       "0  23  240   357  120  120   0   0   0  73  0.5  ...    0    0    0    0    0   \n",
       "1  45    5   632  132  132   4   0   4  17  2.1  ...    0    0    0    1    0   \n",
       "2  45  177   779  133  133   2   0   5  16  2.1  ...    0    0    0    1    0   \n",
       "3  45  411  1192  134  134   2   0   6  16  2.4  ...    0    0    0    1    0   \n",
       "4  45  533  1147  132  132   4   0   5  16  2.4  ...    0    0    0    0    0   \n",
       "\n",
       "   V33  V34  V35  Class  NSP  \n",
       "0    0    1    0      9    2  \n",
       "1    0    0    0      6    1  \n",
       "2    0    0    0      6    1  \n",
       "3    0    0    0      6    1  \n",
       "4    0    0    0      2    1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./cardiotocography_csv.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aOH_GFGZZFx"
   },
   "source": [
    "## Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "id": "OkUM_mnHhudC",
    "outputId": "b4e542fe-5d03-4602-e4d9-06a6d0e7c65d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   V1   V2    V3   V4   V5  V6  V7  V8  V9  V10  ...  V28  V29  V30  V31  V32  \\\n",
      "0  23  240   357  120  120   0   0   0  73  0.5  ...    0    0    0    0    0   \n",
      "1  45    5   632  132  132   4   0   4  17  2.1  ...    0    0    0    1    0   \n",
      "2  45  177   779  133  133   2   0   5  16  2.1  ...    0    0    0    1    0   \n",
      "3  45  411  1192  134  134   2   0   6  16  2.4  ...    0    0    0    1    0   \n",
      "4  45  533  1147  132  132   4   0   5  16  2.4  ...    0    0    0    0    0   \n",
      "\n",
      "   V33  V34  V35  Class  NSP  \n",
      "0    0    1    0      9    2  \n",
      "1    0    0    0      6    1  \n",
      "2    0    0    0      6    1  \n",
      "3    0    0    0      6    1  \n",
      "4    0    0    0      2    1  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
      "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
      "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31',\n",
      "       'V32', 'V33', 'V34', 'V35', 'Class', 'NSP'],\n",
      "      dtype='object')\n",
      "                V1           V2           V3           V4           V5  \\\n",
      "count  2126.000000  2126.000000  2126.000000  2126.000000  2126.000000   \n",
      "mean     25.122766   878.439793  1702.877234   133.303857   133.303857   \n",
      "std      15.241753   894.084748   930.919143     9.840844     9.840844   \n",
      "min       1.000000     0.000000   287.000000   106.000000   106.000000   \n",
      "25%      10.000000    55.000000  1009.000000   126.000000   126.000000   \n",
      "50%      29.000000   538.000000  1241.000000   133.000000   133.000000   \n",
      "75%      39.000000  1521.000000  2434.750000   140.000000   140.000000   \n",
      "max      48.000000  3296.000000  3599.000000   160.000000   160.000000   \n",
      "\n",
      "                V6           V7           V8           V9          V10  ...  \\\n",
      "count  2126.000000  2126.000000  2126.000000  2126.000000  2126.000000  ...   \n",
      "mean      2.722484     7.241298     3.659925    46.990122     1.332785  ...   \n",
      "std       3.560850    37.125309     2.847094    17.192814     0.883241  ...   \n",
      "min       0.000000     0.000000     0.000000    12.000000     0.200000  ...   \n",
      "25%       0.000000     0.000000     1.000000    32.000000     0.700000  ...   \n",
      "50%       1.000000     0.000000     3.000000    49.000000     1.200000  ...   \n",
      "75%       4.000000     2.000000     5.000000    61.000000     1.700000  ...   \n",
      "max      26.000000   564.000000    23.000000    87.000000     7.000000  ...   \n",
      "\n",
      "               V28          V29          V30          V31          V32  \\\n",
      "count  2126.000000  2126.000000  2126.000000  2126.000000  2126.000000   \n",
      "mean      0.024929     0.038100     0.033866     0.156162     0.118532   \n",
      "std       0.155947     0.191482     0.180928     0.363094     0.323314   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "               V33          V34          V35        Class          NSP  \n",
      "count  2126.000000  2126.000000  2126.000000  2126.000000  2126.000000  \n",
      "mean      0.050329     0.032455     0.092662     4.509878     1.304327  \n",
      "std       0.218675     0.177248     0.290027     3.026883     0.614377  \n",
      "min       0.000000     0.000000     0.000000     1.000000     1.000000  \n",
      "25%       0.000000     0.000000     0.000000     2.000000     1.000000  \n",
      "50%       0.000000     0.000000     0.000000     4.000000     1.000000  \n",
      "75%       0.000000     0.000000     0.000000     7.000000     1.000000  \n",
      "max       1.000000     1.000000     1.000000    10.000000     3.000000  \n",
      "\n",
      "[8 rows x 37 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2126 entries, 0 to 2125\n",
      "Data columns (total 37 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   V1      2126 non-null   int64  \n",
      " 1   V2      2126 non-null   int64  \n",
      " 2   V3      2126 non-null   int64  \n",
      " 3   V4      2126 non-null   int64  \n",
      " 4   V5      2126 non-null   int64  \n",
      " 5   V6      2126 non-null   int64  \n",
      " 6   V7      2126 non-null   int64  \n",
      " 7   V8      2126 non-null   int64  \n",
      " 8   V9      2126 non-null   int64  \n",
      " 9   V10     2126 non-null   float64\n",
      " 10  V11     2126 non-null   int64  \n",
      " 11  V12     2126 non-null   float64\n",
      " 12  V13     2126 non-null   int64  \n",
      " 13  V14     2126 non-null   int64  \n",
      " 14  V15     2126 non-null   int64  \n",
      " 15  V16     2126 non-null   int64  \n",
      " 16  V17     2126 non-null   int64  \n",
      " 17  V18     2126 non-null   int64  \n",
      " 18  V19     2126 non-null   int64  \n",
      " 19  V20     2126 non-null   int64  \n",
      " 20  V21     2126 non-null   int64  \n",
      " 21  V22     2126 non-null   int64  \n",
      " 22  V23     2126 non-null   int64  \n",
      " 23  V24     2126 non-null   int64  \n",
      " 24  V25     2126 non-null   int64  \n",
      " 25  V26     2126 non-null   int64  \n",
      " 26  V27     2126 non-null   int64  \n",
      " 27  V28     2126 non-null   int64  \n",
      " 28  V29     2126 non-null   int64  \n",
      " 29  V30     2126 non-null   int64  \n",
      " 30  V31     2126 non-null   int64  \n",
      " 31  V32     2126 non-null   int64  \n",
      " 32  V33     2126 non-null   int64  \n",
      " 33  V34     2126 non-null   int64  \n",
      " 34  V35     2126 non-null   int64  \n",
      " 35  Class   2126 non-null   int64  \n",
      " 36  NSP     2126 non-null   int64  \n",
      "dtypes: float64(2), int64(35)\n",
      "memory usage: 614.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "# read the first row of the dataset \n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "print(df.describe())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiaaNFX2Zf-I"
   },
   "source": [
    "## Clean/transform data (where necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "3JuJlVGDkINJ",
    "outputId": "082781c3-db3c-44e8-a7fd-ba35f35cbbfc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
       "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
       "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31',\n",
       "       'V32', 'V33', 'V34', 'V35', 'Class', 'NSP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# based on findings from data exploration, we need to clean up colum names, as there are some leading whitespace characters\n",
    "df.columns = [s.strip() for s in df.columns] \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the columns we are not using as predictors (see previous notebooks -- we are given a subset of input variables to consider). The Class variable is a target variable and has 10 classes. But, we are using the NSP variable as our target variable. So we can drop the Class variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The V25 variable has 3 categories such as 0, 1 and 2. We can encode this variable,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "      <th>V31</th>\n",
       "      <th>V32</th>\n",
       "      <th>V33</th>\n",
       "      <th>V34</th>\n",
       "      <th>V35</th>\n",
       "      <th>NSP</th>\n",
       "      <th>V25_0</th>\n",
       "      <th>V25_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>240</td>\n",
       "      <td>357</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>632</td>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>177</td>\n",
       "      <td>779</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   V1   V2   V3   V4   V5  V6  V7  V8  V9  V10  ...  V29  V30  V31  V32  V33  \\\n",
       "0  23  240  357  120  120   0   0   0  73  0.5  ...    0    0    0    0    0   \n",
       "1  45    5  632  132  132   4   0   4  17  2.1  ...    0    0    1    0    0   \n",
       "2  45  177  779  133  133   2   0   5  16  2.1  ...    0    0    1    0    0   \n",
       "\n",
       "   V34  V35  NSP  V25_0  V25_1  \n",
       "0    1    0    2      0      1  \n",
       "1    0    0    1      1      0  \n",
       "2    0    0    1      1      0  \n",
       "\n",
       "[3 rows x 37 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# translation V25 categories into dummy vars\n",
    "df = df.join(pd.get_dummies(df['V25'], prefix='V25', drop_first=True))\n",
    "df.drop('V25', axis=1, inplace = True)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V1       0\n",
       "V2       0\n",
       "V3       0\n",
       "V4       0\n",
       "V5       0\n",
       "V6       0\n",
       "V7       0\n",
       "V8       0\n",
       "V9       0\n",
       "V10      0\n",
       "V11      0\n",
       "V12      0\n",
       "V13      0\n",
       "V14      0\n",
       "V15      0\n",
       "V16      0\n",
       "V17      0\n",
       "V18      0\n",
       "V19      0\n",
       "V20      0\n",
       "V21      0\n",
       "V22      0\n",
       "V23      0\n",
       "V24      0\n",
       "V26      0\n",
       "V27      0\n",
       "V28      0\n",
       "V29      0\n",
       "V30      0\n",
       "V31      0\n",
       "V32      0\n",
       "V33      0\n",
       "V34      0\n",
       "V35      0\n",
       "NSP      0\n",
       "V25_0    0\n",
       "V25_1    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NSP target variable has 3 classes such as Normal (1), Suspect(2), and Pathologic (3). Out of these 3 classes suspect and pathologic are the stage of concerns and need immediate intervention to control the symptoms. So we can combine suspect and pathologic as the suspect class to target on these conditions. We are assigning 1 to suspect/Pathologic and 0 to Normal stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NSP'] = np.where(df['NSP']==1, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "      <th>V31</th>\n",
       "      <th>V32</th>\n",
       "      <th>V33</th>\n",
       "      <th>V34</th>\n",
       "      <th>V35</th>\n",
       "      <th>NSP</th>\n",
       "      <th>V25_0</th>\n",
       "      <th>V25_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>240</td>\n",
       "      <td>357</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>632</td>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>177</td>\n",
       "      <td>779</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>2.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>411</td>\n",
       "      <td>1192</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>2.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>533</td>\n",
       "      <td>1147</td>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>2.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   V1   V2    V3   V4   V5  V6  V7  V8  V9  V10  ...  V29  V30  V31  V32  V33  \\\n",
       "0  23  240   357  120  120   0   0   0  73  0.5  ...    0    0    0    0    0   \n",
       "1  45    5   632  132  132   4   0   4  17  2.1  ...    0    0    1    0    0   \n",
       "2  45  177   779  133  133   2   0   5  16  2.1  ...    0    0    1    0    0   \n",
       "3  45  411  1192  134  134   2   0   6  16  2.4  ...    0    0    1    0    0   \n",
       "4  45  533  1147  132  132   4   0   5  16  2.4  ...    0    0    0    0    0   \n",
       "\n",
       "   V34  V35  NSP  V25_0  V25_1  \n",
       "0    1    0    1      0      1  \n",
       "1    0    0    0      1      0  \n",
       "2    0    0    0      1      0  \n",
       "3    0    0    0      0      1  \n",
       "4    0    0    0      0      1  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKY30W1pZxCP"
   },
   "source": [
    "## Split data intro training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "d0fAfB0ThudG",
    "outputId": "47f231af-3781-4603-95b8-d9a1fca0236e"
   },
   "outputs": [],
   "source": [
    "# construct datasets for analysis\n",
    "target = 'NSP'\n",
    "predictors = list(df.columns)\n",
    "predictors.remove(target)\n",
    "X = df[predictors]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "t0DkCAoChudI",
    "outputId": "4f5824b6-d5e0-419c-c916-6be218916af2"
   },
   "outputs": [],
   "source": [
    "# create the training set and the test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is pharmacutical data and we are targeting suspects from various medical test data, we need to minimize the false negatives as this will cost somebody's life. This cost is significantly high as compared to false positive and in case of false positive the patient/insurance company needs to bear only the further investigation costs. So our measure of score is recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "svmm = SVC()\n",
    "logreg = LogisticRegression()\n",
    "adatree = AdaBoostClassifier()\n",
    "rforest = RandomForestClassifier()\n",
    "xgboost = XGBClassifier()\n",
    "gboost = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Random search of parameter grids of all models ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "The best recall score is 0.9539958592132505\n",
      "... with parameters: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "85 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "85 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.8447205  0.39937888 0.95399586 0.63213251 0.86202899 0.95399586\n",
      " 0.60915114 0.64078675 0.62923395 0.95399586        nan 0.50857143\n",
      "        nan        nan 0.95395445 0.67809524 0.73275362 0.55167702\n",
      " 0.62347826 0.3821118  0.95395445        nan 0.95399586 0.95395445\n",
      " 0.82463768 0.6378882  0.87349896 0.95395445 0.63503106        nan\n",
      " 0.95395445 0.73267081 0.95395445 0.63792961 0.70115942        nan\n",
      " 0.45134576 0.60049689 0.78434783        nan 0.95399586 0.73275362\n",
      " 0.75561077        nan 0.41966874 0.95395445 0.62637681        nan\n",
      " 0.95399586 0.55167702 0.95395445 0.95399586 0.95395445 0.86488613\n",
      " 0.95399586 0.45991718        nan        nan 0.61486542 0.63213251\n",
      "        nan 0.95399586 0.63213251 0.64078675 0.86194617 0.86194617\n",
      " 0.77855072 0.95399586 0.45706004 0.75556936 0.70405797        nan\n",
      " 0.95399586 0.61486542 0.95399586 0.43122153        nan 0.52869565\n",
      "        nan 0.64078675 0.72985507 0.95399586 0.59763975 0.95399586\n",
      " 0.563147   0.95395445 0.95395445 0.95399586        nan 0.40517598\n",
      " 0.87060041 0.95395445 0.95399586 0.63213251 0.62347826        nan\n",
      " 0.52869565 0.77287785 0.77569358 0.95395445]\n",
      "  warnings.warn(\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.8584642  0.40374926 0.97126686 0.63506356 0.87859003 0.97055001\n",
      " 0.60633042 0.64367861 0.6314716  0.97055001        nan 0.51653387\n",
      "        nan        nan 0.96623604 0.70115263 0.74566927 0.55603775\n",
      " 0.62356824 0.38793739 0.96623604        nan 0.97055001 0.96623604\n",
      " 0.85414507 0.64511488 0.89296305 0.96623604 0.6400918         nan\n",
      " 0.96623604 0.73061551 0.96623604 0.64224234 0.73205178        nan\n",
      " 0.45975864 0.59698822 0.78807406        nan 0.97055001 0.74065135\n",
      " 0.77727238        nan 0.43606147 0.96623604 0.62788221        nan\n",
      " 0.97055001 0.55316263 0.96623604 0.97126686 0.96623604 0.8678064\n",
      " 0.97055001 0.46695289        nan        nan 0.61566231 0.63506356\n",
      "        nan 0.97055001 0.63506356 0.64439545 0.87569686 0.87425286\n",
      " 0.79741368 0.97126686 0.46335577 0.76220314 0.70691318        nan\n",
      " 0.97126686 0.61566231 0.97126686 0.44827364        nan 0.52945772\n",
      "        nan 0.64439545 0.74137593 0.97126686 0.59052113 0.97126686\n",
      " 0.5668188  0.96623604 0.96623604 0.97126686        nan 0.41380315\n",
      " 0.89151646 0.96623604 0.97198628 0.63793868 0.62140997        nan\n",
      " 0.52802145 0.7808695  0.79024007 0.96623604]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Grid for Logistic Regression\n",
    "\n",
    "param_grid_logr = [{\n",
    "     'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "     'solver': ['saga'],\n",
    "     'max_iter': np.arange(100,900),},\n",
    "      {\n",
    "     'penalty': ['l1', 'l2'],\n",
    "     'solver': ['liblinear'],\n",
    "     'max_iter': np.arange(100,900),},\n",
    "    {\n",
    "     'penalty': ['l2', 'none'],\n",
    "     'solver': ['lbfgs'],\n",
    "     'max_iter': np.arange(100,900),}\n",
    "]    \n",
    "\n",
    "rand_search_logr = RandomizedSearchCV(estimator = logreg, param_distributions=param_grid_logr, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Logistic Regression model fit for grid search\n",
    "_ = rand_search_logr.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_logr.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_logr.best_params_}\")\n",
    "\n",
    "bestRecallLogr = rand_search_logr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "The best recall score is 0.9453416149068324\n",
      "... with parameters: {'min_samples_split': 31, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.0021, 'max_leaf_nodes': 43, 'max_depth': 15, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.72401656 0.84186335 0.80488613 0.79039337 0.88530021 0.92815735\n",
      " 0.92530021 0.92815735 0.92244306 0.86484472 0.85341615 0.91101449\n",
      " 0.84766046 0.7152381  0.74443064 0.6578882  0.71536232 0.90530021\n",
      " 0.85341615 0.73809524 0.6405383  0.86815735 0.80488613 0.80488613\n",
      " 0.72401656 0.91101449 0.92815735 0.79888199 0.79039337 0.60592133\n",
      " 0.8389234  0.60592133 0.85668737 0.67755694 0.90815735 0.8447205\n",
      "        nan 0.79018634 0.79039337 0.92530021 0.94534161 0.69258799\n",
      " 0.65503106 0.90815735 0.71536232 0.66091097 0.77875776 0.73809524\n",
      " 0.67755694 0.77875776 0.71250518 0.88530021 0.87929607 0.73809524\n",
      " 0.79304348 0.65763975 0.73238095 0.81031056 0.91101449 0.90811594\n",
      " 0.61478261 0.91101449 0.92244306 0.73238095 0.84186335 0.80488613\n",
      " 0.91101449 0.71250518 0.85378882 0.78186335 0.87929607 0.81888199\n",
      " 0.80488613 0.6405383  0.6405383  0.8131677  0.91097308 0.80198758\n",
      " 0.60592133 0.81031056 0.71536232 0.62612836 0.90530021 0.85341615\n",
      " 0.7184265  0.72401656 0.82463768 0.71817805 0.89942029 0.79039337\n",
      " 0.61478261 0.79039337 0.88530021 0.62335404 0.88530021 0.87929607\n",
      " 0.59801242 0.9252588  0.6578882  0.60592133]\n",
      "  warnings.warn(\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.72911219 0.85631882 0.81316108 0.78950517 0.88786777 0.93391094\n",
      " 0.94038576 0.93391094 0.94395194 0.87643434 0.8599211  0.92455842\n",
      " 0.88071736 0.76287099 0.76649906 0.66886878 0.71554369 0.91160878\n",
      " 0.86279879 0.75652252 0.65952399 0.88499007 0.81316108 0.81316108\n",
      " 0.72911219 0.92455842 0.93391094 0.81243392 0.78590805 0.6271731\n",
      " 0.85419922 0.62645368 0.86127227 0.71054124 0.92383899 0.86637787\n",
      "        nan 0.81098734 0.78950517 0.94038576 0.95833011 0.6938604\n",
      " 0.66167453 0.92383899 0.71410742 0.69028132 0.78447693 0.75652252\n",
      " 0.71054124 0.78447693 0.70547175 0.88786777 0.88505969 0.75652252\n",
      " 0.8117145  0.6709729  0.76083907 0.81680462 0.92455842 0.91953018\n",
      " 0.61843944 0.92455842 0.93463036 0.76083907 0.85631882 0.81316108\n",
      " 0.92455842 0.7097883  0.86198912 0.80232072 0.88505969 0.82471829\n",
      " 0.81316108 0.65952399 0.65952399 0.81895516 0.9288827  0.81172739\n",
      " 0.62645368 0.81680462 0.71410742 0.64579304 0.91160878 0.8599211\n",
      " 0.72839277 0.72911219 0.84196127 0.75713365 0.91164488 0.78950517\n",
      " 0.61843944 0.78590805 0.88786777 0.64867074 0.88786777 0.88505969\n",
      " 0.57823419 0.94612052 0.66886878 0.62645368]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Grid for decision tree\n",
    "param_grid_tree = {\n",
    "    'min_samples_split': np.arange(1,100),  \n",
    "    'min_samples_leaf': np.arange(1,100),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 50), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "rand_search_tree = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid_tree, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Decision tree model fit for grid search\n",
    "_ = rand_search_tree.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_tree.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_tree.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 15 is smaller than n_iter=50. Running 15 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "The best recall score is 0.9712629399585921\n",
      "... with parameters: {'kernel': 'linear', 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "# Grid for SVM\n",
    "param_grid_svm = [{\n",
    "    'degree': [2,3],\n",
    "    'C': [1,5,10],\n",
    "    'kernel': ['poly'],   \n",
    "},\n",
    "{\n",
    "    'C': [1,5,10],\n",
    "    'gamma': [1, 0.1],\n",
    "    'kernel': ['rbf'],   \n",
    "},\n",
    "{\n",
    "    'C': [1,5,10],\n",
    "    'kernel': ['linear'],  \n",
    "}]\n",
    "\n",
    "rand_search_svm = RandomizedSearchCV(estimator = svmm, param_distributions=param_grid_svm, cv=kfolds, n_iter=50,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# SVM model fit for grid search\n",
    "_ = rand_search_svm.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_svm.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_svm.best_params_}\")\n",
    "\n",
    "bestRecallSvm = rand_search_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 20 is smaller than n_iter=500. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "The best recall score is 0.959792960662526\n",
      "... with parameters: {'n_estimators': 1000, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#Grid for ADABoost Classifier\n",
    "\n",
    "param_grid_ada = {  \n",
    "     'n_estimators': [10,50,250,1000,2000],\n",
    "     'learning_rate': [0.01,0.1,0.2,1.0],}   \n",
    "\n",
    "rand_search_ada = RandomizedSearchCV(estimator = adatree, param_distributions=param_grid_ada, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# ADABoost Classifier model fit for grid search\n",
    "_ = rand_search_ada.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_ada.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_ada.best_params_}\")\n",
    "\n",
    "bestRecallAda = rand_search_ada.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 180 is smaller than n_iter=500. Running 180 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "300 fits failed out of a total of 900.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "300 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.83610766 0.81614907 0.80459627 0.80169772 0.79879917 0.8363147\n",
      " 0.80455487 0.81035197 0.80451346 0.80169772 0.80161491 0.79594203\n",
      " 0.79594203 0.79598344 0.80165631 0.88517598 0.92244306 0.93097308\n",
      " 0.93681159 0.92811594 0.93391304 0.92244306 0.93387164 0.92811594\n",
      " 0.93101449 0.87076605 0.9136646  0.90227743 0.90513458 0.90231884\n",
      " 0.92534161 0.93962733 0.94538302 0.94538302 0.94538302 0.92240166\n",
      " 0.93966874 0.94538302 0.94538302 0.94538302 0.92240166 0.93681159\n",
      " 0.94828157 0.94538302 0.94538302 0.94538302 0.95399586 0.95399586\n",
      " 0.95113872 0.95399586 0.94538302 0.95109731 0.95113872 0.95113872\n",
      " 0.95399586 0.94244306 0.94534161 0.95399586 0.95399586 0.95399586\n",
      " 0.81325052 0.82194617 0.81594203 0.81884058 0.80451346 0.83345756\n",
      " 0.8189234  0.8189234  0.81308489 0.80455487 0.81921325 0.78447205\n",
      " 0.79879917 0.79879917 0.80165631 0.91668737 0.9194617  0.92240166\n",
      " 0.92240166 0.92811594 0.90223602 0.91668737 0.91664596 0.92231884\n",
      " 0.92240166 0.89374741 0.90799172 0.8994617  0.90795031 0.89362319\n",
      " 0.90505176 0.94538302 0.94819876 0.94248447 0.94534161 0.92807453\n",
      " 0.94248447 0.94534161 0.94824017 0.94819876 0.91374741 0.94256729\n",
      " 0.94538302 0.94248447 0.94534161 0.93958592 0.95403727 0.95689441\n",
      " 0.95689441 0.95689441 0.94828157 0.95975155 0.95689441 0.95689441\n",
      " 0.95689441 0.92815735 0.94819876 0.95399586 0.95399586 0.95399586\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.8412599  0.82543771 0.83475929 0.83188933 0.83260875 0.84982079\n",
      " 0.82329233 0.8397901  0.83549161 0.82398855 0.81896547 0.82184575\n",
      " 0.8211057  0.82398597 0.81895774 0.92528042 0.94612052 0.94539594\n",
      " 0.94468167 0.94396225 0.93822233 0.94036513 0.94467652 0.94468167\n",
      " 0.94396225 0.91233594 0.93247467 0.9346252  0.94037286 0.94180913\n",
      " 0.96767489 0.96120781 0.96623862 0.96336092 0.96407777 0.96983059\n",
      " 0.96551404 0.96335577 0.96479719 0.96336092 0.96265181 0.96408293\n",
      " 0.96479719 0.96264408 0.96264408 0.9813388  0.98850726 0.98922153\n",
      " 0.98993837 0.99209665 0.98491529 0.98994095 0.99066037 0.99137722\n",
      " 0.98994095 0.98491529 0.98635672 0.98850726 0.98922153 0.98994095\n",
      " 0.83116475 0.84191228 0.83691756 0.83692014 0.83619556 0.83904231\n",
      " 0.83549161 0.84912457 0.83548387 0.83547356 0.81463345 0.8146515\n",
      " 0.81679946 0.81967458 0.82039401 0.94180913 0.94611537 0.94324283\n",
      " 0.94396225 0.94396225 0.93175524 0.94252856 0.94396225 0.94468167\n",
      " 0.94396225 0.92886981 0.93317862 0.93461489 0.94037286 0.94252598\n",
      " 0.95978185 0.97341224 0.97485624 0.97629509 0.97629509 0.97053454\n",
      " 0.97700936 0.97701452 0.97629509 0.97629509 0.96837369 0.97845079\n",
      " 0.97413682 0.97557567 0.97557567 0.98993579 0.99424976 0.99569119\n",
      " 0.99497434 0.99497176 0.9913798  0.99209922 0.99281349 0.99497176\n",
      " 0.99569119 0.99138238 0.99353292 0.99209665 0.99353549 0.99425492\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.9597515527950311\n",
      "... with parameters: {'n_estimators': 50, 'max_features': 'sqrt', 'max_depth': 10, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "#Grid for Randomforest Classifier\n",
    "\n",
    "param_grid_rf = {  \n",
    "     'n_estimators': [10,50,250,1000,2000],\n",
    "     'max_features': ['auto', 'sqrt', 'log2'],\n",
    "     'max_depth' : [4,6,8,10],\n",
    "     'criterion' :['gini', 'entropy', 'log_loss'],}   \n",
    "\n",
    "rand_search_rf = RandomizedSearchCV(estimator = rforest, param_distributions=param_grid_rf, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# ADABoost Classifier model fit for grid search\n",
    "_ = rand_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_rf.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_rf.best_params_}\")\n",
    "\n",
    "bestRecallRf = rand_search_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 200 is smaller than n_iter=500. Running 200 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "The best recall score is 0.9655072463768116\n",
      "... with parameters: {'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.2}\n"
     ]
    }
   ],
   "source": [
    "#Grid for XGBoost Classifier\n",
    "\n",
    "param_grid_xg = {  \n",
    "    'max_depth': range (2, 10, 1),\n",
    "    'n_estimators': [10,50,250,1000,2000],\n",
    "    'learning_rate': [1.0,0.2,0.1, 0.01, 0.05],}   \n",
    "\n",
    "rand_search_xg = RandomizedSearchCV(estimator = xgboost, param_distributions=param_grid_xg, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# XGBoost Classifier model fit for grid search\n",
    "_ = rand_search_xg.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_xg.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_xg.best_params_}\")\n",
    "\n",
    "bestRecallXg = rand_search_xg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "960 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "860 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 525, in fit\n",
      "    self._check_params()\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 282, in _check_params\n",
      "    raise ValueError(\"Loss '{0:s}' not supported. \".format(self.loss))\n",
      "ValueError: Loss 'log_loss' not supported. \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "100 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 586, in fit\n",
      "    n_stages = self._fit_stages(\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 663, in _fit_stages\n",
      "    raw_predictions = self._fit_stage(\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 246, in _fit_stage\n",
      "    tree.fit(X, residual, sample_weight=sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1315, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.95689441 0.95395445        nan 0.31312629 0.9252588         nan\n",
      "        nan 0.91101449        nan        nan 0.93387164        nan\n",
      "        nan 0.95689441 0.92236025        nan        nan 0.93391304\n",
      " 0.9426087  0.92815735 0.         0.                nan        nan\n",
      " 0.94256729        nan 0.95689441 0.70703934 0.95399586        nan\n",
      " 0.88782609        nan        nan 0.87937888        nan 0.93672878\n",
      "        nan        nan 0.94538302 0.89080745        nan 0.93681159\n",
      "        nan        nan 0.9626501  0.93681159        nan        nan\n",
      "        nan 0.75598344        nan        nan 0.91084886        nan\n",
      " 0.9626087  0.92521739 0.79884058 0.95403727 0.92815735 0.\n",
      " 0.95979296        nan 0.92815735 0.94542443 0.9194617  0.94832298\n",
      "        nan 0.         0.86207039        nan 0.9252588  0.94538302\n",
      " 0.95113872 0.95975155 0.91378882        nan 0.9626501  0.91089027\n",
      " 0.93672878        nan 0.95693582 0.95693582 0.9310559  0.94828157\n",
      "        nan 0.90811594        nan        nan        nan 0.95693582\n",
      "        nan 0.95407867        nan 0.94538302 0.93681159        nan\n",
      "        nan 0.93966874 0.92244306 0.95693582        nan        nan\n",
      "        nan 0.52033126 0.9626501  0.9252588  0.9426087  0.95979296\n",
      "        nan        nan        nan 0.91374741 0.8994617         nan\n",
      " 0.90811594        nan 0.92236025 0.92815735 0.         0.94828157\n",
      " 0.92815735        nan 0.         0.93962733 0.92240166 0.91954451\n",
      " 0.91950311 0.95407867        nan 0.93097308 0.95693582        nan\n",
      "        nan 0.89080745 0.93971014 0.92815735        nan 0.95689441\n",
      " 0.91950311 0.90811594 0.9626501  0.91101449 0.93395445        nan\n",
      "        nan        nan        nan        nan        nan 0.9252588\n",
      " 0.6610352  0.94824017        nan 0.85635611 0.91093168 0.95975155\n",
      " 0.81010352 0.92815735 0.81010352 0.95113872        nan 0.91660455\n",
      "        nan 0.81010352        nan 0.95693582 0.9310559  0.94542443\n",
      "        nan        nan        nan 0.91954451 0.92815735 0.92240166\n",
      "        nan 0.90811594 0.                nan 0.92240166 0.95979296\n",
      " 0.95118012 0.91101449        nan 0.9626087         nan        nan\n",
      " 0.9252588  0.95693582 0.9310559  0.92236025 0.95403727 0.77026915\n",
      " 0.95689441 0.95689441        nan        nan 0.8821118         nan\n",
      " 0.89652174        nan        nan        nan 0.94530021        nan\n",
      " 0.                nan 0.95975155 0.95403727        nan 0.95689441\n",
      "        nan 0.91101449 0.76443064 0.9252588         nan        nan\n",
      " 0.         0.7299793  0.91668737        nan 0.92236025 0.93101449\n",
      " 0.92815735 0.95403727 0.95693582 0.                nan 0.90811594\n",
      "        nan 0.95971014 0.95975155 0.95113872 0.93966874 0.95403727\n",
      "        nan 0.90811594 0.9052588  0.88786749 0.9626501  0.95689441\n",
      "        nan 0.95979296 0.80111801        nan 0.90811594 0.\n",
      "        nan        nan 0.9252588  0.91101449        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.95693582\n",
      " 0.93387164        nan        nan        nan 0.95979296        nan\n",
      " 0.94542443        nan 0.95693582 0.9252588         nan 0.94542443\n",
      " 0.7699793  0.94828157 0.89954451        nan        nan 0.95113872\n",
      " 0.95693582        nan        nan        nan        nan        nan\n",
      " 0.91668737 0.87933747 0.95975155 0.93391304 0.95403727        nan\n",
      " 0.92815735 0.         0.76977226 0.9252588         nan 0.94828157\n",
      " 0.94252588        nan        nan        nan        nan        nan\n",
      " 0.90811594 0.90811594 0.         0.9626501  0.90803313        nan\n",
      "        nan 0.95403727 0.         0.7873706  0.         0.92236025\n",
      " 0.956853          nan 0.96550725 0.91668737        nan        nan\n",
      "        nan 0.                nan        nan 0.94542443        nan\n",
      "        nan 0.95979296        nan 0.9626087  0.91093168 0.\n",
      "        nan 0.94252588 0.95399586        nan 0.93966874 0.91093168\n",
      " 0.                nan        nan        nan 0.         0.90811594\n",
      " 0.         0.84182195 0.95979296 0.84467909        nan 0.95689441\n",
      "        nan 0.9626501  0.93391304 0.96256729        nan 0.92240166\n",
      "        nan 0.94828157 0.92244306 0.956853   0.95113872        nan\n",
      "        nan        nan 0.74712215 0.89652174        nan        nan\n",
      " 0.91954451 0.90811594 0.7500207  0.93672878 0.94244306 0.95403727\n",
      "        nan 0.95407867 0.         0.94252588 0.91101449 0.956853\n",
      "        nan        nan 0.85929607        nan        nan        nan\n",
      " 0.93101449 0.92815735        nan 0.9052588  0.93677019 0.95118012\n",
      "        nan 0.93962733        nan 0.95689441 0.9252588  0.91374741\n",
      "        nan        nan 0.9626501         nan 0.95979296 0.95693582\n",
      "        nan 0.91101449        nan 0.94538302 0.91383023 0.\n",
      "        nan 0.95975155        nan        nan        nan 0.92240166\n",
      " 0.94538302        nan 0.92530021 0.90811594 0.95403727 0.34741201\n",
      " 0.95403727 0.9252588  0.95399586 0.9626087         nan 0.93101449\n",
      " 0.93391304 0.92811594        nan 0.48571429        nan 0.94542443\n",
      " 0.93971014 0.95399586        nan 0.95399586        nan 0.95979296\n",
      " 0.90811594 0.87648033        nan 0.95975155 0.89660455        nan\n",
      " 0.91954451        nan 0.92240166 0.93677019 0.94252588        nan\n",
      "        nan        nan        nan        nan        nan 0.90240166\n",
      "        nan 0.95979296 0.77855072 0.94828157 0.94828157 0.93391304\n",
      " 0.95689441 0.9252588  0.93966874 0.                nan        nan\n",
      " 0.                nan 0.                nan 0.95979296 0.9310559\n",
      " 0.95395445 0.93391304 0.94538302 0.91660455 0.91378882        nan\n",
      "        nan        nan        nan 0.95979296        nan        nan\n",
      " 0.95979296 0.74716356        nan 0.95689441 0.95979296 0.9626087\n",
      "        nan 0.9626501         nan        nan 0.95975155 0.95975155\n",
      " 0.95689441        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\scbis\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.99856373 0.98635414        nan 0.30744437 0.92959697        nan\n",
      "        nan 0.92455842        nan        nan 0.95976896        nan\n",
      "        nan 0.99497176 0.93463552        nan        nan 0.94755421\n",
      " 0.95760295 0.93391094 0.         0.                nan        nan\n",
      " 0.99856373        nan 0.97413682 0.71406617 0.99928315        nan\n",
      " 0.91021119        nan        nan 0.89366958        nan 0.94109487\n",
      "        nan        nan 0.98347902 0.91307083        nan 0.94683479\n",
      "        nan        nan 0.99712746 0.94683221        nan        nan\n",
      "        nan 0.76721333        nan        nan 0.91595111        nan\n",
      " 0.99712746 0.94325056 0.80388592 0.99712746 0.93894175 0.\n",
      " 0.99712746        nan 0.94539594 0.98852273 0.9296047  0.99928058\n",
      "        nan 0.         0.87282432        nan 0.94755937 0.99784431\n",
      " 0.97342255 0.99712746 0.91953276        nan 0.99712746 0.92602305\n",
      " 0.957621          nan 0.99856373 0.99712746 0.94108197 0.97986385\n",
      "        nan 0.92240788        nan        nan        nan 0.99712746\n",
      "        nan 0.99784431        nan 0.95545757 0.9482659         nan\n",
      "        nan 0.95617699 0.94108971 0.99784431        nan        nan\n",
      "        nan 0.53450143 0.99712746 0.92959697 0.98132591 0.99784431\n",
      "        nan        nan        nan 0.91595111 0.90373894        nan\n",
      " 0.92096903        nan 0.93102808 0.93247209 0.         0.99784431\n",
      " 0.93031381        nan 0.         0.96263634 0.92886981 0.926005\n",
      " 0.93247209 0.99640803        nan 0.93175524 0.99712746        nan\n",
      "        nan 0.91307083 0.95617442 0.93391094        nan 0.99640803\n",
      " 0.92528558 0.92240788 0.99712746 0.92455842 0.94037028        nan\n",
      "        nan        nan        nan        nan        nan 0.92959697\n",
      " 0.66517882 0.97055517        nan 0.86999046 0.925283   0.99712746\n",
      " 0.82186122 0.92958923 0.82617777 0.97629251        nan 0.92097935\n",
      "        nan 0.82617777        nan 0.99712746 0.94755421 0.96623346\n",
      "        nan        nan        nan 0.92959697 0.95114102 0.9346252\n",
      "        nan 0.92096903 0.                nan 0.93031381 0.99712746\n",
      " 0.99712746 0.92455842        nan 0.99712746        nan        nan\n",
      " 0.92384673 0.99712746 0.94467652 0.93534978 0.99856373 0.76290452\n",
      " 0.99712746 0.99712746        nan        nan 0.89726155        nan\n",
      " 0.90948918        nan        nan        nan 0.96910858        nan\n",
      " 0.                nan 0.99569119 0.98347387        nan 0.99712746\n",
      "        nan 0.92455842 0.79453598 0.92959697        nan        nan\n",
      " 0.         0.74063588 0.92168846        nan 0.94109487 0.94683221\n",
      " 0.93821975 0.99928058 0.99712746 0.                nan 0.92240788\n",
      "        nan 0.99712746 0.99712746 0.99784688 0.95257987 0.99497434\n",
      "        nan 0.92096903 0.9109074  0.88866455 0.99784431 0.98922153\n",
      "        nan 0.99712746 0.81979578        nan 0.92096903 0.\n",
      "        nan        nan 0.93032155 0.92455842        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.99712746\n",
      " 0.94108713        nan        nan        nan 0.99856373        nan\n",
      " 0.9705397         nan 0.99712746 0.93678348        nan 0.96911116\n",
      " 0.80250122 0.99928315 0.91091256        nan        nan 0.96982543\n",
      " 0.99712746        nan        nan        nan        nan        nan\n",
      " 0.92815554 0.89584848 0.9806039  0.94539336 0.99712746        nan\n",
      " 0.93031381 0.         0.79172791 0.9367809         nan 0.96983059\n",
      " 0.95258245        nan        nan        nan        nan        nan\n",
      " 0.92096903 0.92096903 0.         0.99712746 0.926005          nan\n",
      "        nan 0.99712746 0.         0.81319718 0.         0.93247467\n",
      " 0.99784431        nan 0.99712746 0.94610763        nan        nan\n",
      "        nan 0.                nan        nan 0.97844563        nan\n",
      "        nan 0.99712746        nan 0.99856373 0.92385189 0.\n",
      "        nan 0.95617442 0.97557825        nan 0.9532993  0.92385189\n",
      " 0.                nan        nan        nan 0.         0.92240272\n",
      " 0.         0.86999046 0.99784431 0.86999046        nan 0.9734174\n",
      "        nan 0.99856373 0.94539852 0.99712746        nan 0.9346252\n",
      "        nan 0.96120523 0.95041644 0.99712746 0.99784688        nan\n",
      "        nan        nan 0.77085171 0.90015214        nan        nan\n",
      " 0.92815812 0.92240788 0.76581573 0.95043191 0.99712746 0.99712746\n",
      "        nan 0.99856115 0.         0.96192981 0.92455842 0.99712746\n",
      "        nan        nan 0.89007246        nan        nan        nan\n",
      " 0.94037028 0.93821201        nan 0.91377994 0.9597638  0.99856373\n",
      "        nan 0.95833011        nan 0.99784431 0.92672443 0.92097935\n",
      "        nan        nan 0.99712746        nan 0.99928058 0.99712746\n",
      "        nan 0.92455842        nan 0.9827596  0.93535236 0.\n",
      "        nan 0.99784688        nan        nan        nan 0.92456873\n",
      " 0.97628736        nan 0.9504216  0.92096903 0.99784688 0.33981331\n",
      " 0.99712746 0.92959697 0.99712746 0.99928058        nan 0.93677574\n",
      " 0.94395709 0.92816585        nan 0.50715299        nan 0.96839174\n",
      " 0.95329414 0.99712746        nan 0.9827596         nan 0.99712746\n",
      " 0.92168846 0.89079188        nan 0.99712746 0.90445063        nan\n",
      " 0.93247982        nan 0.92959697 0.9619195  0.9626415         nan\n",
      "        nan        nan        nan        nan        nan 0.91089709\n",
      "        nan 0.99712746 0.7938475  0.9805807  0.979147   0.94755421\n",
      " 0.99712746 0.92959697 0.95617699 0.                nan        nan\n",
      " 0.                nan 0.                nan 0.99856373 0.94180398\n",
      " 0.99856373 0.95761069 0.95546015 0.92169103 0.92025734        nan\n",
      "        nan        nan        nan 0.99712746        nan        nan\n",
      " 0.98922153 0.76365746        nan 0.99856373 0.99712746 0.99712746\n",
      "        nan 0.99712746        nan        nan 0.99784431 0.99712746\n",
      " 0.99712746        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best recall score is 0.9655072463768116\n",
      "... with parameters: {'n_estimators': 250, 'min_samples_split': 15, 'min_samples_leaf': 7, 'min_impurity_decrease': 0.0041, 'max_depth': 5, 'loss': 'deviance', 'learning_rate': 0.2, 'criterion': 'friedman_mse'}\n"
     ]
    }
   ],
   "source": [
    "#Grid for Gradient Boost Classifier\n",
    "\n",
    "param_grid_gb = {  \n",
    "    'min_samples_split': np.arange(1,20),  \n",
    "    'min_samples_leaf': np.arange(1,12),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005), \n",
    "    'loss': ['log_loss', 'deviance', 'exponential'],\n",
    "    'criterion': ['friedman_mse', 'squared_error'],\n",
    "    'max_depth': range (2, 10, 1),\n",
    "    'n_estimators': [10,50,250,1000,2000],\n",
    "    'learning_rate': [1.0,0.2,0.1, 0.01, 0.05],}   \n",
    "\n",
    "rand_search_gb = RandomizedSearchCV(estimator = gboost, param_distributions=param_grid_gb, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Gradient Boost Classifier model fit for grid search\n",
    "_ = rand_search_gb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search_gb.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_gb.best_params_}\")\n",
    "\n",
    "bestRecallgb = rand_search_gb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================LOGISTIC REGRESSION====================\n",
      "\n",
      "The best recall score is 0.9539958592132505\n",
      "... with parameters: {'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 200}\n",
      "\n",
      "=========================DECISION TREE==========================\n",
      "\n",
      "The best recall score is 0.9453416149068324\n",
      "... with parameters: {'min_samples_split': 31, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.0021, 'max_leaf_nodes': 43, 'max_depth': 15, 'criterion': 'entropy'}\n",
      "\n",
      "==============================SVM===============================\n",
      "\n",
      "The best recall score is 0.9712629399585921\n",
      "... with parameters: {'kernel': 'linear', 'C': 1}\n",
      "\n",
      "=========================ADABOOST===============================\n",
      "\n",
      "The best recall score is 0.959792960662526\n",
      "... with parameters: {'n_estimators': 1000, 'learning_rate': 0.1}\n",
      "\n",
      "=========================RANDOMFOREST===========================\n",
      "\n",
      "The best recall score is 0.9597515527950311\n",
      "... with parameters: {'n_estimators': 50, 'max_features': 'sqrt', 'max_depth': 10, 'criterion': 'entropy'}\n",
      "\n",
      "=========================XGBOOST================================\n",
      "\n",
      "The best recall score is 0.9655072463768116\n",
      "... with parameters: {'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.2}\n",
      "\n",
      "=========================GRADIENT BOOST================================\n",
      "\n",
      "The best recall score is 0.9655072463768116\n",
      "... with parameters: {'n_estimators': 250, 'min_samples_split': 15, 'min_samples_leaf': 7, 'min_impurity_decrease': 0.0041, 'max_depth': 5, 'loss': 'deviance', 'learning_rate': 0.2, 'criterion': 'friedman_mse'}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=========================LOGISTIC REGRESSION====================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_logr.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_logr.best_params_}\")\n",
    "print(\"\\n=========================DECISION TREE==========================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_tree.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_tree.best_params_}\")\n",
    "print(\"\\n==============================SVM===============================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_svm.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_svm.best_params_}\")\n",
    "print(\"\\n=========================ADABOOST===============================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_ada.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_ada.best_params_}\")\n",
    "print(\"\\n=========================RANDOMFOREST===========================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_rf.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_rf.best_params_}\")\n",
    "print(\"\\n=========================XGBOOST================================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_xg.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_xg.best_params_}\")\n",
    "print(\"\\n=========================GRADIENT BOOST================================\\n\")\n",
    "print(f\"The best {score_measure} score is {rand_search_gb.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search_gb.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Grid Search ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "The best recall score is 0.959792960662526\n",
      "... with parameters: {'learning_rate': 0.07, 'n_estimators': 800}\n"
     ]
    }
   ],
   "source": [
    "# ADABoosting classifier grid\n",
    "param_grid_ada = {  \n",
    "     'n_estimators': [800,1000,1200],\n",
    "     'learning_rate': [0.07,0.1,0.13],}  \n",
    "\n",
    "\n",
    "grid_search_ada = GridSearchCV(estimator = adatree, param_grid=param_grid_ada, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# ADABoosting classifier fit\n",
    "_ = grid_search_ada.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_ada.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_ada.best_params_}\")\n",
    "\n",
    "bestRecallAda = grid_search_ada.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "The best recall score is 0.9655072463768116\n",
      "... with parameters: {'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "#Grid for XGBoost Classifier\n",
    "\n",
    "param_grid_xg = {  \n",
    "    'max_depth': [2,4,6],\n",
    "    'n_estimators': [200,250,300],\n",
    "    'learning_rate': [0.17,0.2,0.23],}   \n",
    "\n",
    "grid_search_xg = GridSearchCV(estimator = xgboost, param_grid=param_grid_xg, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# XGBoost Classifier model fit for grid search\n",
    "_ = grid_search_xg.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_xg.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_xg.best_params_}\")\n",
    "\n",
    "bestRecallXg = grid_search_xg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "The best recall score is 0.9568944099378882\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "#Grid for Randomforest Classifier\n",
    "param_grid_rf = {  \n",
    "     'n_estimators': [40,50,60],\n",
    "     'max_features': ['sqrt'],\n",
    "     'max_depth' : [8,10,12],\n",
    "     'criterion' :['entropy'],}   \n",
    "\n",
    "grid_search_rf = GridSearchCV(estimator = rforest, param_grid=param_grid_rf, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# ADABoost Classifier model fit for grid search\n",
    "_ = grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_rf.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_rf.best_params_}\")\n",
    "\n",
    "bestRecallRf = grid_search_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "The best recall score is 0.9712629399585921\n",
      "... with parameters: {'C': 1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Grid for SVM\n",
    "param_grid_svm = {\n",
    "    'C': [1,2,3],\n",
    "    'kernel': ['linear'],  \n",
    "}\n",
    "\n",
    "grid_search_svm = GridSearchCV(estimator = svmm, param_grid=param_grid_svm, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# SVM model fit for grid search\n",
    "_ = grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_svm.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_svm.best_params_}\")\n",
    "\n",
    "bestRecallSvm = grid_search_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "The best recall score is 0.94824016563147\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 13, 'max_leaf_nodes': 41, 'min_impurity_decrease': 0.0018, 'min_samples_leaf': 2, 'min_samples_split': 29}\n"
     ]
    }
   ],
   "source": [
    "# Grid for decision tree\n",
    " \n",
    "param_grid_tree = {\n",
    "    'min_samples_split': [29,31,33],  \n",
    "    'min_samples_leaf': [2,4,6],\n",
    "    'min_impurity_decrease': [0.0018,0.0021,0.0024],\n",
    "    'max_leaf_nodes': [41,43,45], \n",
    "    'max_depth': [13,15,17], \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "grid_search_tree = GridSearchCV(estimator = dtree, param_grid=param_grid_tree, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Decision tree model fit for grid search\n",
    "_ = grid_search_tree.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_tree.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_tree.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "The best recall score is 0.9539958592132505\n",
      "... with parameters: {'max_iter': 180, 'penalty': 'l1', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "#Grid for Logistic Regression\n",
    "\n",
    "param_grid_logr = {\n",
    "     'penalty': ['l1'],\n",
    "     'solver': ['liblinear'],\n",
    "     'max_iter': [180,200,220],\n",
    "}\n",
    "\n",
    "grid_search_logr = GridSearchCV(estimator = logreg, param_grid=param_grid_logr, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Logistic Regression model fit for grid search\n",
    "_ = grid_search_logr.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_logr.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_logr.best_params_}\")\n",
    "\n",
    "bestRecallLogr = grid_search_logr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
      "The best recall score is 0.9655072463768116\n",
      "... with parameters: {'criterion': 'friedman_mse', 'learning_rate': 0.17, 'loss': 'deviance', 'max_depth': 7, 'min_impurity_decrease': 0.0038, 'min_samples_leaf': 5, 'min_samples_split': 17, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "#Grid for Gradient Boost Classifier\n",
    "\n",
    "param_grid_gb = {  \n",
    "    'min_samples_split': [13,15,17],  \n",
    "    'min_samples_leaf': [5,7,9],\n",
    "    'min_impurity_decrease': [0.0038,0.0041,0.0044], \n",
    "    'loss': ['deviance'],\n",
    "    'criterion': ['friedman_mse'],\n",
    "    'max_depth': [3,5,7],\n",
    "    'n_estimators': [200,250,300],\n",
    "    'learning_rate': [0.17,0.2,0.23],}   \n",
    "\n",
    "grid_search_gb = GridSearchCV(estimator = gboost, param_grid=param_grid_gb, cv=kfolds,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "# Gradient Boost Classifier model fit for grid search\n",
    "_ = grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search_gb.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search_gb.best_params_}\")\n",
    "\n",
    "bestRecallgb = rand_search_gb.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final models with best parameters found ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(criterion='entropy', max_depth=13, max_leaf_nodes=41, min_impurity_decrease=0.0018, min_samples_leaf=2, min_samples_split=29)\n",
    "svmm = SVC(C=1, kernel='linear')\n",
    "logreg = LogisticRegression(max_iter=180, penalty='l1', solver='liblinear')\n",
    "adatree = AdaBoostClassifier(learning_rate=0.07, n_estimators=800)\n",
    "rforest = RandomForestClassifier(criterion='entropy', max_depth=10, max_features='sqrt', n_estimators=50)\n",
    "xgboost = XGBClassifier(learning_rate=0.2, max_depth=4, n_estimators=250)\n",
    "gboost = GradientBoostingClassifier(criterion='friedman_mse', learning_rate=0.17, loss='deviance', max_depth=7, min_impurity_decrease=0.0038, min_samples_leaf=5, min_samples_split=17, n_estimators=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fit for train dataset and prediction with test dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.959349593495935\n",
      "Accuracy Score:   0.9887218045112782\n",
      "Precision Score:  0.9915966386554622\n",
      "F1 Score:         0.9752066115702479\n"
     ]
    }
   ],
   "source": [
    "_ = xgboost.fit(X_train, y_train)\n",
    "y_pred = xgboost.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "xgboost_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.967479674796748\n",
      "Accuracy Score:   0.9906015037593985\n",
      "Precision Score:  0.9916666666666667\n",
      "F1 Score:         0.9794238683127573\n"
     ]
    }
   ],
   "source": [
    "_ = rforest.fit(X_train, y_train)\n",
    "y_pred = rforest.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "rforest_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.983739837398374\n",
      "Accuracy Score:   0.9962406015037594\n",
      "Precision Score:  1.0\n",
      "F1 Score:         0.9918032786885246\n"
     ]
    }
   ],
   "source": [
    "_ = adatree.fit(X_train, y_train)\n",
    "y_pred = adatree.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "adatree_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.975609756097561\n",
      "Accuracy Score:   0.9906015037593985\n",
      "Precision Score:  0.9836065573770492\n",
      "F1 Score:         0.9795918367346939\n"
     ]
    }
   ],
   "source": [
    "_ = logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "logreg_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.975609756097561\n",
      "Accuracy Score:   0.9943609022556391\n",
      "Precision Score:  1.0\n",
      "F1 Score:         0.9876543209876543\n"
     ]
    }
   ],
   "source": [
    "_ = svmm.fit(X_train, y_train)\n",
    "y_pred = svmm.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "svmm_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.975609756097561\n",
      "Accuracy Score:   0.9868421052631579\n",
      "Precision Score:  0.967741935483871\n",
      "F1 Score:         0.97165991902834\n"
     ]
    }
   ],
   "source": [
    "_ = dtree.fit(X_train, y_train)\n",
    "y_pred = dtree.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "dtree_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model             Score       \n",
      "************************************\n",
      ">> Recall Score:  0.975609756097561\n",
      "Accuracy Score:   0.9943609022556391\n",
      "Precision Score:  1.0\n",
      "F1 Score:         0.9876543209876543\n"
     ]
    }
   ],
   "source": [
    "_ = gboost.fit(X_train, y_train)\n",
    "y_pred = gboost.predict(X_test)\n",
    "\n",
    "print(f\"{'Model':^18}{'Score':^18}\")\n",
    "print(\"************************************\")\n",
    "print(f\"{'>> Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "\n",
    "gboost_recall = recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall scores...\n",
      "Decision Tree:           0.975609756097561\n",
      "SVM:                     0.975609756097561\n",
      "Logistic Regression      0.975609756097561\n",
      "Random Forest:           0.967479674796748\n",
      "Ada Boosted Tree:        0.983739837398374\n",
      "XGBoost Tree:            0.959349593495935\n",
      "Gradient Boosting        0.975609756097561\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall scores...\")\n",
    "print(f\"{'Decision Tree:':25}{dtree_recall}\")\n",
    "print(f\"{'SVM:':25}{svmm_recall}\")\n",
    "print(f\"{'Logistic Regression':25}{logreg_recall}\")\n",
    "\n",
    "print(f\"{'Random Forest:':25}{rforest_recall}\")\n",
    "print(f\"{'Ada Boosted Tree:':25}{adatree_recall}\")\n",
    "print(f\"{'XGBoost Tree:':25}{xgboost_recall}\")\n",
    "print(f\"{'Gradient Boosting':25}{gboost_recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Models ##\n",
    "\n",
    "The analysis of recall score of models show that ADABoost Tree is the best model (0.984). \n",
    "While XGBoost performs worst (0.96), the Decision Tree, SVM, Logistic Regression, and Gradient Boosting perform same (0.976). The random forest performs better than XGBoost (.967), but not better than most of the models.\n",
    "\n",
    "This shows that the esemble models perform better than the pruning models."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Class08b-decision_tree_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b056086e24cb5602cbcb82122035cd3d6ee2ccbf5df29c16e348c108b0f83be3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
